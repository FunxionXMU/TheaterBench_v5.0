import os
import json
import base64
import re
import time
import subprocess
from tqdm import tqdm
from openai import OpenAI
from prettytable import PrettyTable
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®éƒ¨åˆ† =================

# APIé…ç½®ï¼ŒæŒ‰æ¨¡å‹æä¾›å•†åˆ†ç±»
API_CONFIGS = {
    "aliyun": {
        "api_key": "sk-7e6b3a62b1f64945a5a4a9347afa5c72",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    },
    "siliconflow": {
        "api_key": "sk-izrxbwrnxotwsvcngnnmwivxmyqukrivnjcoszzpscmfasjz",
        "base_url": "https://api.siliconflow.cn/v1"  # ä¿®æ­£ï¼šå»æ‰äº†æœ«å°¾çš„ /chat/completions
    }
}

# é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
TEST_MODELS = [
    # é˜¿é‡Œäº‘æ¨¡å‹
    "qwen-vl-max-latest",
    "qwen3-vl-30b-a3b-thinking",
    "qwen3-vl-plus",
    "qwen3-vl-flash",
    # SiliconFlowæ¨¡å‹
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    "zai-org/GLM-4.6V"
]

# æ¨¡å‹åˆ°APIæä¾›å•†çš„æ˜ å°„
MODEL_TO_API = {
    "qwen3-vl-plus": "aliyun",
    "qwen3-vl-flash": "aliyun",
    "qwen-vl-max-latest": "aliyun",
    "qwen3-vl-30b-a3b-thinking": "aliyun",
    "Qwen/Qwen3-Omni-30B-A3B-Instruct": "siliconflow",
    "zai-org/GLM-4.6V": "siliconflow",
    "Qwen/Qwen3-VL-30B-A3B-Instruct": "siliconflow"
}

VIDEO_DIR = "t2v_videos"

# æœ€å¤§å¹¶å‘æ•°è®¾ç½®ï¼ˆæ ¹æ®APIé™åˆ¶å’Œç³»ç»Ÿèµ„æºè°ƒæ•´ï¼‰
# è®¾ç½®ä¸ºæ¨¡å‹æ€»æ•°+1ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘èƒ½å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹
MAX_CONCURRENT_MODELS = len(TEST_MODELS) + 1

# ================= è¾…åŠ©å‡½æ•° =================

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    return name.replace(" ", "_")

def compress_video_smart(video_path, target_size_mb=7.0):
    """
    æ™ºèƒ½å‹ç¼©è§†é¢‘ï¼šå¦‚æœè§†é¢‘è¶…è¿‡ç›®æ ‡å¤§å°ï¼Œåˆ™é€šè¿‡é™ä½å¸§ç‡æ¥å‹ç¼©ï¼Œ
    åŒæ—¶ä¿æŒç”»è´¨ï¼ˆCRF 18ï¼‰ä¸å˜ã€‚
    
    å…³é”®è°ƒæ•´ï¼š
    æˆ‘ä»¬å°† target_size_mb é»˜è®¤å€¼ä» 9.5MB é™ä½åˆ° 7.0MBã€‚
    åŸå› ï¼šBase64 ç¼–ç ä¼šä½¿æ–‡ä»¶å¤§å°å¢åŠ çº¦ 33%ã€‚
    - 7.0MB * 1.33 â‰ˆ 9.3MB (å®‰å…¨ï¼Œå°äº API çš„ 10MB é™åˆ¶)
    - 9.0MB * 1.33 â‰ˆ 12.0MB (ä¼šæŠ¥é”™ Exceeded limit)
    
    è¿”å›: (æ–‡ä»¶è·¯å¾„, æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶)
    """
    if not os.path.exists(video_path):
        return video_path, False

    file_size = os.path.getsize(video_path) / (1024 * 1024)
    
    # ä¸ç®¡æ–‡ä»¶å¤§å°ï¼Œéƒ½è¿›è¡Œå‹ç¼©
    print(f"ğŸ“¦ Video {os.path.basename(video_path)} is {file_size:.2f}MB. Compressing to optimize for API...")
    
    temp_path = video_path.replace(".mp4", "_compressed_temp.mp4")
    
    # ç­–ç•¥ï¼šé™ä½å¸§ç‡ï¼Œä½†ä¿æŒé«˜è´¨é‡
    # è°ƒæ•´ä¸º 5fps (1ç§’è§†é¢‘ä¹Ÿæœ‰5å¸§ï¼Œæ»¡è¶³ >4å¸§ çš„è¦æ±‚)
    target_fps = "5" 
    
    # å¦‚æœæ–‡ä»¶ç‰¹åˆ«å¤§ (>30MB)ï¼Œå°è¯•é™ä½åˆ° 3fps (å¯¹äºé•¿è§†é¢‘ä¹Ÿè¶³å¤Ÿ)
    if file_size > 30:
        target_fps = "3"

    cmd = [
        "ffmpeg", "-y",                # è¦†ç›–è¾“å‡º
        "-i", video_path,              # è¾“å…¥
        "-r", target_fps,              # ç›®æ ‡å¸§ç‡
        "-c:v", "libx264",             # ç¼–ç å™¨
        "-pix_fmt", "yuv420p",         # å¼ºåˆ¶ä½¿ç”¨ yuv420p åƒç´ æ ¼å¼
        "-crf", "18",                  # æ’å®šè´¨é‡å› å­ (18=é«˜ç”»è´¨/è§†è§‰æ— æŸ)
        "-preset", "veryfast",         # ç¼–ç é€Ÿåº¦
        "-an",                         # ç§»é™¤éŸ³é¢‘
        temp_path
    ]
    
    try:
        # æ‰§è¡Œå‹ç¼©ï¼Œé™é»˜è¾“å‡º
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        
        # æ£€æŸ¥å‹ç¼©åçš„å¤§å°
        new_size = os.path.getsize(temp_path) / (1024 * 1024)
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œå°è¯•è¿›ä¸€æ­¥å‹ç¼© (CRF 23 ä»ç„¶å¾ˆæ¸…æ™°)
        if new_size > target_size_mb:
            print(f"âš ï¸ Still too large ({new_size:.2f}MB). Re-compressing aggressively...")
            cmd_aggressive = [
                "ffmpeg", "-y", "-i", video_path,
                "-r", "2", "-c:v", "libx264", # åªæœ‰åœ¨å¤§æ–‡ä»¶(é€šå¸¸è¾ƒé•¿)æ‰æ•¢ç”¨2fps
                "-pix_fmt", "yuv420p",
                "-crf", "23", "-preset", "veryfast", "-an",
                temp_path
            ]
            subprocess.run(cmd_aggressive, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
            new_size = os.path.getsize(temp_path) / (1024 * 1024)

        print(f"âœ… Compressed to {new_size:.2f}MB (FPS: {target_fps})")
        return temp_path, True
        
    except Exception as e:
        print(f"âŒ Compression failed (ffmpeg might be missing): {e}. Using original.")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return video_path, False

def call_transit_model(raw_text):
    """è°ƒç”¨ä¸­è½¬æ¨¡å‹è½¬æ¢æ ¼å¼"""
    transit_system_prompt = """
    You are a professional text format converter. Your job is to convert the given text into a strict JSON format.
    The JSON should have two fields:
    1. "answer": The selected option letter (e.g., "A", "B", "C", "D")
    2. "reasoning": "<Short explanation>"
    OUTPUT FORMAT: Return a STRICT JSON object with no additional text.
    """
    
    transit_user_prompt = f"Please convert the following text into JSON format:\n{raw_text}"
    
    transit_messages = [
        {"role": "system", "content": transit_system_prompt},
        {"role": "user", "content": transit_user_prompt}
    ]
    
    api_config = API_CONFIGS["siliconflow"]
    transit_client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    for retry in range(3):
        try:
            response = transit_client.chat.completions.create(
                model="Qwen/Qwen3-Next-80B-A3B-Instruct",
                messages=transit_messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"},
                timeout=60
            )
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
        except Exception as e:
            if retry == 2: return {"answer": "N/A", "reasoning": f"Transit Error: {e}"}
            time.sleep(2)
    return {"answer": "N/A", "reasoning": "Transit failed"}

def call_vlm_model(video_path, question, options, model_name, conversation_history=None):
    """è°ƒç”¨ VLM æ¨¡å‹ï¼Œæ”¯æŒä¸¤æ¬¡æé—®ï¼šç¬¬ä¸€æ¬¡åªæé—®ï¼Œç¬¬äºŒæ¬¡å¸¦è§†é¢‘å’Œå†å²è®°å½•"""
    SUPPORT_JSON_MODELS = ["Qwen/Qwen3-VL-30B-A3B-Instruct", "Qwen/Qwen3-Omni-30B-A3B-Instruct", "qwen3-vl-plus", "qwen3-vl-flash", "qwen-vl-max-latest"]
    api_provider = MODEL_TO_API.get(model_name, "siliconflow")
    api_config = API_CONFIGS[api_provider]
    
    client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    system_prompt = """
    You are an expert video understanding AI.
    Analyze the video and return a JSON object: {"answer": "<option>", "reasoning": "<text>"}
    """

    user_prompt = f"""
    # VIDEO QUESTION:
    **Question:** {question}
    **OPTIONS:**
    {chr(10).join([f"{key}. {value}" for key, value in options.items()])}
    Select the correct answer.
    """

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    if conversation_history:
        # å¦‚æœæœ‰å†å²è®°å½•ï¼Œç›´æ¥ä½¿ç”¨å†å²è®°å½•ï¼ˆå·²åŒ…å«ç³»ç»Ÿæç¤ºï¼‰
        messages = conversation_history.copy()
        
        # æ·»åŠ è§†é¢‘å†…å®¹
        try:
            with open(video_path, "rb") as f:
                video_bytes = f.read()
                video_base64 = base64.b64encode(video_bytes).decode('utf-8')
        except FileNotFoundError:
            return {"answer": "N/A", "reasoning": "Video file not found during read"}

        # æ„å»ºè§†é¢‘å†…å®¹éƒ¨åˆ†
        video_url = f"data:video/mp4;base64,{video_base64}"
        
        if api_provider == "aliyun":
            # é˜¿é‡Œäº‘ OpenAI å…¼å®¹æ¨¡å¼æ ¼å¼ (type: video_url)
            video_content = {
                "type": "video_url",
                "video_url": {"url": video_url}
            }
        else:
            # SiliconFlow æ ¼å¼ (å¸¦ fps ç­‰å‚æ•°)
            video_content = {
                "type": "video_url",
                "video_url": {
                    "url": video_url,
                    "detail": "auto",
                    "max_frames": 12,
                    "fps": 1
                }
            }
        
        # æ·»åŠ å¸¦è§†é¢‘çš„æ–°é—®é¢˜
        messages.append({"role": "user", "content": [{"type": "text", "text": user_prompt}, video_content]})
    else:
        # ç¬¬ä¸€æ¬¡æé—®ï¼Œä¸å¸¦è§†é¢‘ï¼Œåªé—®é—®é¢˜
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": [{"type": "text", "text": user_prompt}]}
        ]

    max_retries = 3
    retry_delay = 5
    
    for retry in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"} if model_name in SUPPORT_JSON_MODELS else None,
                timeout=60
            )
            
            content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
            
            if model_name in SUPPORT_JSON_MODELS:
                try:
                    response_data = json.loads(content)
                except json.JSONDecodeError:
                    response_data = call_transit_model(content)
            else:
                response_data = call_transit_model(content)
            
            # è¿”å›å“åº”æ•°æ®å’Œæ›´æ–°åçš„å¯¹è¯å†å²
            updated_history = messages.copy()
            updated_history.append({"role": "assistant", "content": content})
            
            return response_data, updated_history
            
        except Exception as e:
            print(f"âŒ {model_name} Request Failed: {e}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                # å¦‚æœå‡ºé”™ï¼Œä»ç„¶è¿”å›å¯¹è¯å†å²ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
                if conversation_history:
                    updated_history = conversation_history.copy()
                else:
                    updated_history = messages.copy()
                return {"answer": "N/A", "reasoning": f"Exception: {e}"}, updated_history
    
    return {"answer": "N/A", "reasoning": "Max retries exceeded"}, messages

# ================= ä¸»ç¨‹åº =================

def process_video(eval_entry, prompt_index, test_results, lock, processed_videos):
    """å¤„ç†å•ä¸ªè§†é¢‘çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
    object_name = eval_entry['object_name']
    scenario_type = eval_entry['scenario_type']
    
    key = f"{object_name}_{scenario_type}"
    if key not in prompt_index:
        return
    
    if eval_entry['status'] != 'evaluated' or not eval_entry.get('best_video'):
        return
    
    prompt_entry = prompt_index[key]
    mcq = prompt_entry['mcq']
    
    best_video_file = eval_entry['best_video']
    original_video_path = os.path.join(VIDEO_DIR, best_video_file)
    
    # ä½¿ç”¨object_name_scenario_typeç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œé¿å…åŒä¸€è§†é¢‘è¢«é‡å¤å¤„ç†
    unique_key = f"{object_name}_{scenario_type}"
    
    # æ£€æŸ¥è¯¥ç»„åˆæ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
    if unique_key in processed_videos:
        print(f"â­ï¸  Skipping {best_video_file} ({object_name} - {scenario_type})... already processed")
        return
    
    print(f"ğŸ”„ Processing {best_video_file} ({object_name} - {scenario_type})...")
    
    # === è§†é¢‘å‹ç¼©å¤„ç† ===
    current_video_path, is_temp_file = compress_video_smart(original_video_path)
    
    try:
        # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰æ¨¡å‹
        with ThreadPoolExecutor(max_workers=min(MAX_CONCURRENT_MODELS, len(TEST_MODELS))) as executor:
            # æäº¤æ‰€æœ‰æ¨¡å‹è°ƒç”¨ä»»åŠ¡ï¼ˆç¬¬ä¸€æ¬¡æé—®ï¼šä¸å¸¦è§†é¢‘ï¼‰
            future_to_model_first = {
                executor.submit(call_vlm_model, current_video_path, mcq['question'], mcq['options'], model_name): model_name
                for model_name in TEST_MODELS
            }
            
            # æ”¶é›†ç¬¬ä¸€æ¬¡æé—®çš„ç»“æœ
            first_round_results = {}
            for future in as_completed(future_to_model_first):
                model_name = future_to_model_first[future]
                try:
                    first_response, conversation_history = future.result()
                    first_round_results[model_name] = {
                        'response': first_response,
                        'conversation_history': conversation_history
                    }
                except Exception as e:
                    print(f"   [{model_name}] First round error: {e}")
                    first_round_results[model_name] = {
                        'response': {"answer": "N/A", "reasoning": f"First round error: {e}"},
                        'conversation_history': None
                    }
            
            # æäº¤æ‰€æœ‰æ¨¡å‹çš„ç¬¬äºŒæ¬¡æé—®ä»»åŠ¡ï¼ˆå¸¦è§†é¢‘å’Œå†å²è®°å½•ï¼‰
            future_to_model_second = {}
            for model_name in TEST_MODELS:
                first_result = first_round_results.get(model_name)
                if first_result and first_result['conversation_history']:
                    future_to_model_second[executor.submit(
                        call_vlm_model, current_video_path, mcq['question'], mcq['options'], 
                        model_name, first_result['conversation_history']
                    )] = model_name
            
            # æ”¶é›†ç¬¬äºŒæ¬¡æé—®çš„ç»“æœå¹¶æ›´æ–°æ€»ç»“æœ
            for future in as_completed(future_to_model_second):
                model_name = future_to_model_second[future]
                try:
                    second_response, _ = future.result()
                    first_response = first_round_results[model_name]['response']
                    
                    correct_answer = mcq['correct_answer']
                    first_answer = first_response['answer']
                    second_answer = second_response['answer']
                    
                    first_is_correct = str(first_answer).upper() == str(correct_answer).upper()
                    second_is_correct = str(second_answer).upper() == str(correct_answer).upper()
                    
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": mcq['question'],
                            "mcq_options": mcq['options'],
                            "correct_answer": correct_answer,
                            "model_name": model_name,
                            "first_answer": first_answer,
                            "first_is_correct": first_is_correct,
                            "first_reasoning": first_response['reasoning'],
                            "second_answer": second_answer,
                            "second_is_correct": second_is_correct,
                            "second_reasoning": second_response['reasoning'],
                            "original_score": eval_entry['best_score']
                        })
                        # æ ‡è®°è¯¥object_name_scenario_typeç»„åˆå·²å¤„ç†
                        processed_videos.add(unique_key)
                       
                    print(f"   [{model_name}] First: {first_answer} ({'âœ…' if first_is_correct else 'âŒ'}) | Second: {second_answer} ({'âœ…' if second_is_correct else 'âŒ'})")
                except Exception as e:
                    print(f"   [{model_name}] Second round error: {e}")
                    first_response = first_round_results[model_name]['response']
                    correct_answer = mcq['correct_answer']
                    first_answer = first_response['answer']
                    first_is_correct = str(first_answer).upper() == str(correct_answer).upper()
                    
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": mcq['question'],
                            "mcq_options": mcq['options'],
                            "correct_answer": correct_answer,
                            "model_name": model_name,
                            "first_answer": first_answer,
                            "first_is_correct": first_is_correct,
                            "first_reasoning": first_response['reasoning'],
                            "second_answer": "N/A",
                            "second_is_correct": False,
                            "second_reasoning": f"Error: {e}",
                            "original_score": eval_entry['best_score']
                        })
                        # æ ‡è®°è¯¥object_name_scenario_typeç»„åˆå·²å¤„ç†
                        processed_videos.add(unique_key)
    
    finally:
        # === æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ===
        if is_temp_file and os.path.exists(current_video_path):
            try:
                os.remove(current_video_path)
            except OSError:
                pass

def main():
    import threading
    
    print(f"ğŸš€ Starting Video Understanding Test using {', '.join(TEST_MODELS)}...")
    
    eval_files = [f for f in os.listdir('.') if f.startswith('physibench_evaluated_v') and f.endswith('.json')]
    eval_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    if not eval_files:
        print("âŒ No evaluation files found.")
        return
    
    eval_file = eval_files[0]
    version = re.search(r'v(\d+)', eval_file).group(1)
    
    # å¯»æ‰¾å¯¹åº”çš„MCQç»“æœæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒçš„å‘½åæ ¼å¼ï¼ˆå¦‚mcq_blind_test_results_v76_unique.jsonï¼‰
    mcq_files = [f for f in os.listdir('.') if f.startswith('mcq_blind_test_results_') and f.endswith('.json')]
    matched_mcq_files = []
    for f in mcq_files:
        match = re.search(r'v(\d+)', f)
        if match and match.group(1) == version:
            matched_mcq_files.append(f)
    
    if not matched_mcq_files:
        print(f"âŒ No MCQ result files found for version v{version}")
        return
    
    # é€‰æ‹©æœ€æ–°çš„MCQç»“æœæ–‡ä»¶
    mcq_file = max(matched_mcq_files, key=os.path.getmtime)
    
    with open(eval_file, "r", encoding='utf-8') as f: eval_results = json.load(f)
    with open(mcq_file, "r", encoding='utf-8') as f: mcq_results = json.load(f)
    
    # åˆ›å»ºç´¢å¼•ï¼Œæ˜ å°„object_name_scenario_typeåˆ°mcqç»“æœ
    prompt_index = {f"{result['director_data']['object_name']}_{result['director_data']['scenario_type']}": result for result in mcq_results if result.get('blind_test_passed', False)}
    
    test_results = []
    lock = threading.Lock()  # ç”¨äºçº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æµ‹è¯•ç»“æœæ–‡ä»¶ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    output_file = f"video_prior_trap_v{version}.json"
    processed_videos = set()
    
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding='utf-8') as f:
                existing_results = json.load(f)
            test_results = existing_results
            
            # æå–å·²å¤„ç†çš„object_name_scenario_typeç»„åˆ
            for result in existing_results:
                unique_key = f"{result['object_name']}_{result['scenario_type']}"
                processed_videos.add(unique_key)
            
            print(f"ğŸ“‹ Found existing results file: {output_file}")
            print(f"   - {len(existing_results)} results loaded")
            print(f"   - {len(processed_videos)} unique object-scenario combinations already processed")
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  Failed to load existing results: {e}")
            print("   Starting fresh...")
    
    # è®¡ç®—æ€»çš„å¹¶å‘æ•°
    # æ¯ä¸ªè§†é¢‘éœ€è¦å¤„ç†len(TEST_MODELS)ä¸ªæ¨¡å‹ï¼Œæ‰€ä»¥æ€»çš„å¹¶å‘æ•°åº”è¯¥æ˜¯MAX_CONCURRENT_MODELS * è§†é¢‘å¹¶å‘æ•°
    # é™ä½å¹¶å‘æ•°ä»¥é¿å…APIé€Ÿç‡é™åˆ¶ (TPM limit reached)
    video_concurrency = 2
    total_concurrent_workers = video_concurrency
    
    print(f"âš™ï¸  Using {total_concurrent_workers} concurrent video workers, each with up to {MAX_CONCURRENT_MODELS} model workers...")
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰è§†é¢‘
    with ThreadPoolExecutor(max_workers=total_concurrent_workers) as executor:
        # æäº¤æ‰€æœ‰è§†é¢‘å¤„ç†ä»»åŠ¡
        futures = []
        valid_entries = 0
        for eval_entry in eval_results:
            object_name = eval_entry['object_name']
            scenario_type = eval_entry['scenario_type']
            key = f"{object_name}_{scenario_type}"
            
            # åªæäº¤æœ‰æ•ˆçš„è¯„ä¼°æ¡ç›®
            if key in prompt_index and eval_entry['status'] == 'evaluated' and eval_entry.get('best_video'):
                future = executor.submit(process_video, eval_entry, prompt_index, test_results, lock, processed_videos)
                futures.append(future)
                valid_entries += 1
        
        print(f"ğŸ“‹ Found {valid_entries} valid evaluation entries matching MCQ blind test results...")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print(f"ğŸ“‹ Submitted {len(futures)} video processing tasks...")
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Videos"):
            try:
                future.result()
            except Exception as e:
                print(f"âŒ Video processing error: {e}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if test_results:
        # ç»Ÿè®¡ç¬¬ä¸€æ¬¡æé—®çš„æ­£ç¡®ç‡
        first_correct_count = sum(1 for r in test_results if r.get('first_is_correct', False))
        # ç»Ÿè®¡ç¬¬äºŒæ¬¡æé—®çš„æ­£ç¡®ç‡
        second_correct_count = sum(1 for r in test_results if r.get('second_is_correct', False))
        
        print(f"\nğŸ“Š First Round Accuracy: {first_correct_count / len(test_results) * 100:.1f}%")
        print(f"ğŸ“Š Second Round Accuracy: {second_correct_count / len(test_results) * 100:.1f}%")
        
    # ä¿å­˜
    output_file = f"video_prior_trap_v{version}.json"
    with open(output_file, "w", encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Results saved to {output_file}")
    
    # ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡
    if not test_results:
        print("\nâŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç»Ÿè®¡")
        return
    
    print(f"\nğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡...")
    
    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„åœºæ™¯ç±»å‹å’Œæ¨¡å‹åç§°
    scenario_types = sorted(set(r['scenario_type'] for r in test_results))
    model_names = sorted(set(r['model_name'] for r in test_results))
    
    # ç»Ÿè®¡æ•°æ®ç»“æ„: {model: {scenario: (first_correct_count, second_correct_count, total_count)}}
    stats = {}
    for model in model_names:
        stats[model] = {}
        for scenario in scenario_types:
            # ç­›é€‰è¯¥æ¨¡å‹åœ¨è¯¥åœºæ™¯ä¸‹çš„æ‰€æœ‰ç»“æœ
            model_scenario_results = [
                r for r in test_results 
                if r['model_name'] == model and r['scenario_type'] == scenario
            ]
            if not model_scenario_results:
                stats[model][scenario] = (0, 0, 0)
                continue
            
            total = len(model_scenario_results)
            first_correct = sum(1 for r in model_scenario_results if r.get('first_is_correct', False))
            second_correct = sum(1 for r in model_scenario_results if r.get('second_is_correct', False))
            stats[model][scenario] = (first_correct, second_correct, total)
    
    # ä½¿ç”¨ PrettyTable æ‰“å°ç»Ÿè®¡è¡¨æ ¼
    print(f"\n{'='*150}")
    print("ğŸ“ˆ å„æ¨¡å‹åœ¨å„åœºæ™¯ä¸‹çš„æ­£ç¡®ç‡ç»Ÿè®¡ï¼ˆç¬¬ä¸€è½®ï¼šæ— è§†é¢‘ï¼Œç¬¬äºŒè½®ï¼šæœ‰è§†é¢‘ï¼‰")
    print(f"{'='*150}")
    
    # åˆ›å»ºè¡¨æ ¼
    table = PrettyTable()
    
    # è®¾ç½®è¡¨å¤´
    field_names = ["åœºæ™¯ç±»å‹"]
    for model in model_names:
        field_names.append(model + " (ç¬¬ä¸€è½®)")
        field_names.append(model + " (ç¬¬äºŒè½®)")
    field_names.append("æƒ…æ™¯æ€»è®¡ (ç¬¬ä¸€è½®)")
    field_names.append("æƒ…æ™¯æ€»è®¡ (ç¬¬äºŒè½®)")
    table.field_names = field_names
    
    # å¡«å……è¡¨æ ¼å†…å®¹
    for scenario in scenario_types:
        row = [scenario]
        scenario_total_first_correct = 0
        scenario_total_second_correct = 0
        scenario_total_count = 0
        
        for model in model_names:
            first_correct, second_correct, total = stats[model][scenario]
            
            # ç¬¬ä¸€è½®æ­£ç¡®ç‡
            if total == 0:
                first_accuracy = "N/A"
            else:
                first_accuracy = f"{first_correct/total*100:.1f}% ({first_correct}/{total})"
            row.append(first_accuracy)
            
            # ç¬¬äºŒè½®æ­£ç¡®ç‡
            if total == 0:
                second_accuracy = "N/A"
            else:
                second_accuracy = f"{second_correct/total*100:.1f}% ({second_correct}/{total})"
            row.append(second_accuracy)
            
            # ç´¯è®¡åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
            scenario_total_first_correct += first_correct
            scenario_total_second_correct += second_correct
            scenario_total_count += total
        
        # è®¡ç®—åœºæ™¯ç¬¬ä¸€è½®æ€»è®¡
        if scenario_total_count == 0:
            scenario_first_accuracy = "N/A"
        else:
            scenario_first_accuracy = f"{scenario_total_first_correct/scenario_total_count*100:.1f}% ({scenario_total_first_correct}/{scenario_total_count})"
        row.append(scenario_first_accuracy)
        
        # è®¡ç®—åœºæ™¯ç¬¬äºŒè½®æ€»è®¡
        if scenario_total_count == 0:
            scenario_second_accuracy = "N/A"
        else:
            scenario_second_accuracy = f"{scenario_total_second_correct/scenario_total_count*100:.1f}% ({scenario_total_second_correct}/{scenario_total_count})"
        row.append(scenario_second_accuracy)
        
        table.add_row(row)
    
    # æ·»åŠ æ€»è®¡è¡Œ
    total_row = ["æ€»è®¡"]
    overall_total_first_correct = 0
    overall_total_second_correct = 0
    overall_total_count = 0
    
    for model in model_names:
        all_first_correct = sum(first_correct for first_correct, second_correct, total in stats[model].values())
        all_second_correct = sum(second_correct for first_correct, second_correct, total in stats[model].values())
        all_total = sum(total for first_correct, second_correct, total in stats[model].values())
        
        # æ¨¡å‹ç¬¬ä¸€è½®æ€»è®¡
        if all_total == 0:
            model_first_accuracy = "N/A"
        else:
            model_first_accuracy = f"{all_first_correct/all_total*100:.1f}% ({all_first_correct}/{all_total})"
        total_row.append(model_first_accuracy)
        
        # æ¨¡å‹ç¬¬äºŒè½®æ€»è®¡
        if all_total == 0:
            model_second_accuracy = "N/A"
        else:
            model_second_accuracy = f"{all_second_correct/all_total*100:.1f}% ({all_second_correct}/{all_total})"
        total_row.append(model_second_accuracy)
        
        # ç´¯è®¡æ‰€æœ‰åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
        overall_total_first_correct += all_first_correct
        overall_total_second_correct += all_second_correct
        overall_total_count += all_total
    
    # è®¡ç®—æ‰€æœ‰åœºæ™¯æ€»è®¡çš„æ€»è®¡ï¼ˆç¬¬ä¸€è½®ï¼‰
    if overall_total_count == 0:
        overall_first_accuracy = "N/A"
    else:
        overall_first_accuracy = f"{overall_total_first_correct/overall_total_count*100:.1f}% ({overall_total_first_correct}/{overall_total_count})"
    total_row.append(overall_first_accuracy)
    
    # è®¡ç®—æ‰€æœ‰åœºæ™¯æ€»è®¡çš„æ€»è®¡ï¼ˆç¬¬äºŒè½®ï¼‰
    if overall_total_count == 0:
        overall_second_accuracy = "N/A"
    else:
        overall_second_accuracy = f"{overall_total_second_correct/overall_total_count*100:.1f}% ({overall_total_second_correct}/{overall_total_count})"
    total_row.append(overall_second_accuracy)
    
    table.add_row(total_row)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    table.align = "l"  # å·¦å¯¹é½
    table.padding_width = 1  # å•å…ƒæ ¼å†…è¾¹è·
    
    # æ‰“å°è¡¨æ ¼
    print(table)
    print(f"{'='*150}")

if __name__ == "__main__":
    main()