import os
import json
import base64
import re
import time
import subprocess
from tqdm import tqdm
from openai import OpenAI
from prettytable import PrettyTable
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®éƒ¨åˆ† =================

# APIé…ç½®ï¼ŒæŒ‰æ¨¡å‹æä¾›å•†åˆ†ç±»
API_CONFIGS = {
    "aliyun": {
        "api_key": "sk-7e6b3a62b1f64945a5a4a9347afa5c72",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    },
    "siliconflow": {
        "api_key": "sk-izrxbwrnxotwsvcngnnmwivxmyqukrivnjcoszzpscmfasjz",
        "base_url": "https://api.siliconflow.cn/v1"  # ä¿®æ­£ï¼šå»æ‰äº†æœ«å°¾çš„ /chat/completions
    }
}

# é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
TEST_MODELS = [
    # é˜¿é‡Œäº‘æ¨¡å‹
    "qwen-vl-max-latest",
    "qwen3-vl-30b-a3b-thinking",
    "qwen3-vl-plus",
    "qwen3-vl-flash",
    # SiliconFlowæ¨¡å‹
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    "zai-org/GLM-4.6V"
]

# æ¨¡å‹åˆ°APIæä¾›å•†çš„æ˜ å°„
MODEL_TO_API = {
    "qwen3-vl-plus": "aliyun",
    "qwen3-vl-flash": "aliyun",
    "qwen-vl-max-latest": "aliyun",
    "qwen3-vl-30b-a3b-thinking": "aliyun",
    "Qwen/Qwen3-Omni-30B-A3B-Instruct": "siliconflow",
    "zai-org/GLM-4.6V": "siliconflow",
    "Qwen/Qwen3-VL-30B-A3B-Instruct": "siliconflow"
}

VIDEO_DIR = "t2v_videos"

# æœ€å¤§å¹¶å‘æ•°è®¾ç½®ï¼ˆæ ¹æ®APIé™åˆ¶å’Œç³»ç»Ÿèµ„æºè°ƒæ•´ï¼‰
# è®¾ç½®ä¸ºæ¨¡å‹æ€»æ•°+1ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘èƒ½å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹
MAX_CONCURRENT_MODELS = len(TEST_MODELS) + 1

# ================= è¾…åŠ©å‡½æ•° =================

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    return name.replace(" ", "_")

def extract_video_segment(video_path, start_time=3, end_time=7):
    """
    ä»è§†é¢‘ä¸­æå–æŒ‡å®šæ—¶é—´æ®µï¼ˆ3-7ç§’ï¼‰çš„ç‰‡æ®µ
    è¿”å›: (æå–çš„è§†é¢‘ç‰‡æ®µè·¯å¾„, æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶)
    """
    if not os.path.exists(video_path):
        return video_path, False
    
    segment_path = video_path.replace(".mp4", f"_segment_{start_time}s_{end_time}s.mp4")
    
    cmd = [
        "ffmpeg", "-y",                # è¦†ç›–è¾“å‡º
        "-i", video_path,              # è¾“å…¥
        "-ss", str(start_time),        # å¼€å§‹æ—¶é—´
        "-to", str(end_time),          # ç»“æŸæ—¶é—´
        "-c:v", "libx264",             # è§†é¢‘ç¼–ç å™¨
        "-pix_fmt", "yuv420p",         # åƒç´ æ ¼å¼
        "-an",                         # ç§»é™¤éŸ³é¢‘
        segment_path
    ]
    
    try:
        # æ‰§è¡Œæˆªå–ï¼Œé™é»˜è¾“å‡º
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        print(f"âœ… Extracted video segment from {start_time}s to {end_time}s: {segment_path}")
        return segment_path, True
    except Exception as e:
        print(f"âŒ Video segment extraction failed: {e}. Using original video.")
        return video_path, False

def compress_video_smart(video_path, target_size_mb=7.0):
    """
    æ™ºèƒ½å‹ç¼©è§†é¢‘ï¼šå¦‚æœè§†é¢‘è¶…è¿‡ç›®æ ‡å¤§å°ï¼Œåˆ™é€šè¿‡é™ä½å¸§ç‡æ¥å‹ç¼©ï¼Œ
    åŒæ—¶ä¿æŒç”»è´¨ï¼ˆCRF 18ï¼‰ä¸å˜ã€‚
    
    å…³é”®è°ƒæ•´ï¼š
    æˆ‘ä»¬å°† target_size_mb é»˜è®¤å€¼ä» 9.5MB é™ä½åˆ° 7.0MBã€‚
    åŸå› ï¼šBase64 ç¼–ç ä¼šä½¿æ–‡ä»¶å¤§å°å¢åŠ çº¦ 33%ã€‚
    - 7.0MB * 1.33 â‰ˆ 9.3MB (å®‰å…¨ï¼Œå°äº API çš„ 10MB é™åˆ¶)
    - 9.0MB * 1.33 â‰ˆ 12.0MB (ä¼šæŠ¥é”™ Exceeded limit)
    
    è¿”å›: (æ–‡ä»¶è·¯å¾„, æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶)
    """
    if not os.path.exists(video_path):
        return video_path, False

    file_size = os.path.getsize(video_path) / (1024 * 1024)
    
    # ä¸ç®¡æ–‡ä»¶å¤§å°ï¼Œéƒ½è¿›è¡Œå‹ç¼©
    print(f"ğŸ“¦ Video {os.path.basename(video_path)} is {file_size:.2f}MB. Compressing to optimize for API...")
    
    temp_path = video_path.replace(".mp4", "_compressed_temp.mp4")
    
    # ç­–ç•¥ï¼šé™ä½å¸§ç‡ï¼Œä½†ä¿æŒé«˜è´¨é‡
    # è°ƒæ•´ä¸º 5fps (1ç§’è§†é¢‘ä¹Ÿæœ‰5å¸§ï¼Œæ»¡è¶³ >4å¸§ çš„è¦æ±‚)
    target_fps = "5" 
    
    # å¦‚æœæ–‡ä»¶ç‰¹åˆ«å¤§ (>30MB)ï¼Œå°è¯•é™ä½åˆ° 3fps (å¯¹äºé•¿è§†é¢‘ä¹Ÿè¶³å¤Ÿ)
    if file_size > 30:
        target_fps = "3"

    cmd = [
        "ffmpeg", "-y",                # è¦†ç›–è¾“å‡º
        "-i", video_path,              # è¾“å…¥
        "-r", target_fps,              # ç›®æ ‡å¸§ç‡
        "-c:v", "libx264",             # ç¼–ç å™¨
        "-pix_fmt", "yuv420p",         # å¼ºåˆ¶ä½¿ç”¨ yuv420p åƒç´ æ ¼å¼
        "-crf", "18",                  # æ’å®šè´¨é‡å› å­ (18=é«˜ç”»è´¨/è§†è§‰æ— æŸ)
        "-preset", "veryfast",         # ç¼–ç é€Ÿåº¦
        "-an",                         # ç§»é™¤éŸ³é¢‘
        temp_path
    ]
    
    try:
        # æ‰§è¡Œå‹ç¼©ï¼Œé™é»˜è¾“å‡º
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        
        # æ£€æŸ¥å‹ç¼©åçš„å¤§å°
        new_size = os.path.getsize(temp_path) / (1024 * 1024)
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œå°è¯•è¿›ä¸€æ­¥å‹ç¼© (CRF 23 ä»ç„¶å¾ˆæ¸…æ™°)
        if new_size > target_size_mb:
            print(f"âš ï¸ Still too large ({new_size:.2f}MB). Re-compressing aggressively...")
            cmd_aggressive = [
                "ffmpeg", "-y", "-i", video_path,
                "-r", "2", "-c:v", "libx264", # åªæœ‰åœ¨å¤§æ–‡ä»¶(é€šå¸¸è¾ƒé•¿)æ‰æ•¢ç”¨2fps
                "-pix_fmt", "yuv420p",
                "-crf", "23", "-preset", "veryfast", "-an",
                temp_path
            ]
            subprocess.run(cmd_aggressive, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
            new_size = os.path.getsize(temp_path) / (1024 * 1024)

        print(f"âœ… Compressed to {new_size:.2f}MB (FPS: {target_fps})")
        return temp_path, True
        
    except Exception as e:
        print(f"âŒ Compression failed (ffmpeg might be missing): {e}. Using original.")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return video_path, False

def call_transit_model(raw_text):
    """è°ƒç”¨ä¸­è½¬æ¨¡å‹è½¬æ¢æ ¼å¼"""
    transit_system_prompt = """
    You are a professional text format converter. Your job is to convert the given text into a strict JSON format.
    The JSON should have two fields:
    1. "answer": The selected option letter (e.g., "A", "B", "C", "D")
    2. "reasoning": "<Short explanation>"
    OUTPUT FORMAT: Return a STRICT JSON object with no additional text.
    """
    
    transit_user_prompt = f"Please convert the following text into JSON format:\n{raw_text}"
    
    transit_messages = [
        {"role": "system", "content": transit_system_prompt},
        {"role": "user", "content": transit_user_prompt}
    ]
    
    api_config = API_CONFIGS["siliconflow"]
    transit_client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    for retry in range(3):
        try:
            response = transit_client.chat.completions.create(
                model="Qwen/Qwen3-Next-80B-A3B-Instruct",
                messages=transit_messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"},
                timeout=60
            )
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
        except Exception as e:
            if retry == 2: return {"answer": "N/A", "reasoning": f"Transit Error: {e}"}
            time.sleep(2)
    return {"answer": "N/A", "reasoning": "Transit failed"}

def call_vlm_model(video_path, question, options, model_name):
    """è°ƒç”¨ VLM æ¨¡å‹"""
    SUPPORT_JSON_MODELS = ["Qwen/Qwen3-VL-30B-A3B-Instruct", "Qwen/Qwen3-Omni-30B-A3B-Instruct", "qwen3-vl-plus", "qwen3-vl-flash", "qwen-vl-max-latest"]
    api_provider = MODEL_TO_API.get(model_name, "siliconflow")
    api_config = API_CONFIGS[api_provider]
    
    client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    system_prompt = """
    You are an expert video understanding AI. 
    Analyze the video and return a JSON object: {"answer": "<option>", "reasoning": "<text>"}
    """

    user_prompt = f"""
    # VIDEO QUESTION:
    **Question:** {question}
    **OPTIONS:**
    {chr(10).join([f"{key}. {value}" for key, value in options.items()])}
    Select the correct answer.
    """

    try:
        with open(video_path, "rb") as f:
            video_bytes = f.read()
            video_base64 = base64.b64encode(video_bytes).decode('utf-8')
    except FileNotFoundError:
        return {"answer": "N/A", "reasoning": "Video file not found during read"}

    # æ„å»ºè§†é¢‘å†…å®¹éƒ¨åˆ†
    video_url = f"data:video/mp4;base64,{video_base64}"
    
    if api_provider == "aliyun":
        # é˜¿é‡Œäº‘ OpenAI å…¼å®¹æ¨¡å¼æ ¼å¼ (type: video_url)
        video_content = {
            "type": "video_url",
            "video_url": {"url": video_url}
        }
    else:
        # SiliconFlow æ ¼å¼ (å¸¦ fps ç­‰å‚æ•°)
        video_content = {
            "type": "video_url",
            "video_url": {
                "url": video_url,
                "detail": "auto",
                "max_frames": 12,
                "fps": 1
            }
        }

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [{"type": "text", "text": user_prompt}, video_content]}
    ]

    max_retries = 3
    retry_delay = 5
    
    for retry in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"} if model_name in SUPPORT_JSON_MODELS else None,
                timeout=60
            )
            
            content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
            
            if model_name in SUPPORT_JSON_MODELS:
                return json.loads(content)
            else:
                return call_transit_model(content)
                
        except Exception as e:
            print(f"âŒ {model_name} Request Failed: {e}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                return {"answer": "N/A", "reasoning": f"Exception: {e}"}
    
    return {"answer": "N/A", "reasoning": "Max retries exceeded"}

# ================= ä¸»ç¨‹åº =================

def process_video(eval_entry, prompt_index, test_results, lock, processed_videos):
    """å¤„ç†å•ä¸ªè§†é¢‘çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
    object_name = eval_entry['object_name']
    scenario_type = eval_entry['scenario_type']
    
    key = f"{object_name}_{scenario_type}"
    if key not in prompt_index:
        return
    
    if eval_entry['status'] != 'evaluated' or not eval_entry.get('best_video'):
        return
    
    prompt_entry = prompt_index[key]
    mcq = prompt_entry['mcq']
    
    best_video_file = eval_entry['best_video']
    original_video_path = os.path.join(VIDEO_DIR, best_video_file)
    
    # ä½¿ç”¨object_name_scenario_typeç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œé¿å…åŒä¸€è§†é¢‘è¢«é‡å¤å¤„ç†
    unique_key = f"{object_name}_{scenario_type}"
    
    # æ£€æŸ¥è¯¥ç»„åˆæ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
    if unique_key in processed_videos:
        print(f"â­ï¸  Skipping {best_video_file} ({object_name} - {scenario_type})... already processed")
        return
    
    # æå‰å°†è§†é¢‘æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…åç»­å¯èƒ½çš„é‡å¤å¤„ç†
    with lock:
        processed_videos.add(unique_key)
    
    print(f"ğŸ”„ Processing {best_video_file} ({object_name} - {scenario_type})...")
    
    # === è§†é¢‘å¤„ç† ===
    # 1. é¦–å…ˆæå–3-7ç§’çš„è§†é¢‘ç‰‡æ®µ
    segment_path, is_segment_temp = extract_video_segment(original_video_path, start_time=3, end_time=7)
    
    # 2. ç„¶åå‹ç¼©è§†é¢‘ç‰‡æ®µ
    current_video_path, is_compressed_temp = compress_video_smart(segment_path)
    
    # ä½¿ç”¨å›ºå®šé—®é¢˜
    question = "What most likely happened according to the video?"
    
    try:
        # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰æ¨¡å‹
        with ThreadPoolExecutor(max_workers=min(MAX_CONCURRENT_MODELS, len(TEST_MODELS))) as executor:
            # æäº¤æ‰€æœ‰æ¨¡å‹è°ƒç”¨ä»»åŠ¡
            future_to_model = {
                executor.submit(call_vlm_model, current_video_path, question, mcq['options'], model_name): model_name
                for model_name in TEST_MODELS
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    model_response = future.result()
                    
                    correct_answer = mcq['correct_answer']
                    model_answer = model_response['answer']
                    is_correct = str(model_answer).upper() == str(correct_answer).upper()
                    
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": question,  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„é—®é¢˜
                            "mcq_options": mcq['options'],
                            "correct_answer": correct_answer,
                            "model_name": model_name,
                            "model_answer": model_answer,
                            "is_correct": is_correct,
                            "model_reasoning": model_response['reasoning'],
                            "original_score": eval_entry['best_score']
                        })
                        
                    print(f"   [{model_name}] Ans: {model_answer} ({'âœ…' if is_correct else 'âŒ'})")
                except Exception as e:
                    print(f"   [{model_name}] Error: {e}")
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": question,  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„é—®é¢˜
                            "mcq_options": mcq['options'],
                            "correct_answer": mcq['correct_answer'],
                            "model_name": model_name,
                            "model_answer": "N/A",
                            "is_correct": False,
                            "model_reasoning": f"Error: {e}",
                            "original_score": eval_entry['best_score']
                        })
    
    finally:
        # === æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ===
        temp_files = []
        if is_compressed_temp and current_video_path != segment_path:
            temp_files.append(current_video_path)
        if is_segment_temp and segment_path != original_video_path:
            temp_files.append(segment_path)
            
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except OSError:
                    pass

def main():
    import threading
    
    print(f"ğŸš€ Starting Video Understanding Test using {', '.join(TEST_MODELS)}...")
    print("ğŸ” æµ‹è¯•é…ç½®: æˆªå–è§†é¢‘3-7ç§’ç‰‡æ®µï¼Œä½¿ç”¨å›ºå®šé—®é¢˜")
    
    eval_files = [f for f in os.listdir('.') if f.startswith('physibench_evaluated_v') and f.endswith('.json')]
    eval_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    if not eval_files:
        print("âŒ No evaluation files found.")
        return
    
    eval_file = eval_files[0]
    version = re.search(r'v(\d+)', eval_file).group(1)
    
    # å¯»æ‰¾å¯¹åº”çš„MCQç»“æœæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒçš„å‘½åæ ¼å¼ï¼ˆå¦‚mcq_blind_test_results_v76_unique.jsonï¼‰
    mcq_files = [f for f in os.listdir('.') if f.startswith('mcq_blind_test_results_') and f.endswith('.json')]
    matched_mcq_files = []
    for f in mcq_files:
        match = re.search(r'v(\d+)', f)
        if match and match.group(1) == version:
            matched_mcq_files.append(f)
    
    if not matched_mcq_files:
        print(f"âŒ No MCQ result files found for version v{version}")
        return
    
    # é€‰æ‹©æœ€æ–°çš„MCQç»“æœæ–‡ä»¶
    mcq_file = max(matched_mcq_files, key=os.path.getmtime)
    
    with open(eval_file, "r", encoding='utf-8') as f: eval_results = json.load(f)
    with open(mcq_file, "r", encoding='utf-8') as f: mcq_results = json.load(f)
    
    # åˆ›å»ºç´¢å¼•ï¼Œæ˜ å°„object_name_scenario_typeåˆ°mcqç»“æœ
    prompt_index = {f"{result['director_data']['object_name']}_{result['director_data']['scenario_type']}": result for result in mcq_results if result.get('blind_test_passed', False)}
    
    test_results = []
    lock = threading.Lock()  # ç”¨äºçº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    output_file = f"mid_understanding_test_v{version}.json"
    processed_videos = set()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æµ‹è¯•ç»“æœæ–‡ä»¶ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding='utf-8') as f:
                existing_results = json.load(f)
            test_results = existing_results
            
            # æå–å·²å¤„ç†çš„object_name_scenario_typeç»„åˆ
            for result in existing_results:
                unique_key = f"{result['object_name']}_{result['scenario_type']}"
                processed_videos.add(unique_key)
            
            print(f"ğŸ“‹ Found existing results file: {output_file}")
            print(f"   - {len(existing_results)} results loaded")
            print(f"   - {len(processed_videos)} unique object-scenario combinations already processed")
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  Failed to load existing results: {e}")
            print("   Starting fresh...")
    
    # è®¡ç®—æ€»çš„å¹¶å‘æ•°
    # æ¯ä¸ªè§†é¢‘éœ€è¦å¤„ç†len(TEST_MODELS)ä¸ªæ¨¡å‹ï¼Œæ‰€ä»¥æ€»çš„å¹¶å‘æ•°åº”è¯¥æ˜¯MAX_CONCURRENT_MODELS * è§†é¢‘å¹¶å‘æ•°
    # é™ä½å¹¶å‘æ•°ä»¥é¿å…APIé€Ÿç‡é™åˆ¶ (TPM limit reached)
    video_concurrency = 2
    total_concurrent_workers = video_concurrency
    
    print(f"âš™ï¸  Using {total_concurrent_workers} concurrent video workers, each with up to {MAX_CONCURRENT_MODELS} model workers...")
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰è§†é¢‘
    with ThreadPoolExecutor(max_workers=total_concurrent_workers) as executor:
        # æäº¤æ‰€æœ‰è§†é¢‘å¤„ç†ä»»åŠ¡
        futures = []
        valid_entries = 0
        for eval_entry in eval_results:
            object_name = eval_entry['object_name']
            scenario_type = eval_entry['scenario_type']
            key = f"{object_name}_{scenario_type}"
            
            # åªæäº¤æœ‰æ•ˆçš„è¯„ä¼°æ¡ç›®
            if key in prompt_index and eval_entry['status'] == 'evaluated' and eval_entry.get('best_video'):
                future = executor.submit(process_video, eval_entry, prompt_index, test_results, lock, processed_videos)
                futures.append(future)
                valid_entries += 1
        
        print(f"ğŸ“‹ Found {valid_entries} valid evaluation entries matching MCQ blind test results...")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print(f"ğŸ“‹ Submitted {len(futures)} video processing tasks...")
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Videos"):
            try:
                future.result()
            except Exception as e:
                print(f"âŒ Video processing error: {e}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if test_results:
        correct_count = sum(1 for r in test_results if r['is_correct'])
        print(f"\nğŸ“Š Overall Accuracy: {correct_count / len(test_results) * 100:.1f}%")
        
    # ä¿å­˜
    with open(output_file, "w", encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Results saved to {output_file}")
    
    # ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡
    if not test_results:
        print("\nâŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç»Ÿè®¡")
        return
    
    print(f"\nğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡...")
    
    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„åœºæ™¯ç±»å‹å’Œæ¨¡å‹åç§°
    scenario_types = sorted(set(r['scenario_type'] for r in test_results))
    model_names = sorted(set(r['model_name'] for r in test_results))
    
    # ç»Ÿè®¡æ•°æ®ç»“æ„: {model: {scenario: (correct_count, total_count)}}
    stats = {}
    for model in model_names:
        stats[model] = {}
        for scenario in scenario_types:
            # ç­›é€‰è¯¥æ¨¡å‹åœ¨è¯¥åœºæ™¯ä¸‹çš„æ‰€æœ‰ç»“æœ
            model_scenario_results = [
                r for r in test_results 
                if r['model_name'] == model and r['scenario_type'] == scenario
            ]
            if not model_scenario_results:
                stats[model][scenario] = (0, 0)
                continue
            
            total = len(model_scenario_results)
            correct = sum(1 for r in model_scenario_results if r['is_correct'])
            stats[model][scenario] = (correct, total)
    
    # ä½¿ç”¨ PrettyTable æ‰“å°ç»Ÿè®¡è¡¨æ ¼
    print(f"\n{'='*100}")
    print("ğŸ“ˆ å„æ¨¡å‹åœ¨å„åœºæ™¯ä¸‹çš„æ­£ç¡®ç‡ç»Ÿè®¡")
    print(f"{'='*100}")
    
    # åˆ›å»ºè¡¨æ ¼
    table = PrettyTable()
    
    # è®¾ç½®è¡¨å¤´
    field_names = ["åœºæ™¯ç±»å‹"]
    for model in model_names:
        field_names.append(model)
    field_names.append("æƒ…æ™¯æ€»è®¡")  # æ·»åŠ æƒ…æ™¯æ€»è®¡åˆ—
    table.field_names = field_names
    
    # å¡«å……è¡¨æ ¼å†…å®¹
    for scenario in scenario_types:
        row = [scenario]
        scenario_total_correct = 0
        scenario_total_count = 0
        
        for model in model_names:
            correct, total = stats[model][scenario]
            if total == 0:
                accuracy = "N/A"
            else:
                accuracy = f"{correct/total*100:.1f}% ({correct}/{total})"
            row.append(accuracy)
            
            # ç´¯è®¡åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
            scenario_total_correct += correct
            scenario_total_count += total
        
        # è®¡ç®—åœºæ™¯æ€»è®¡
        if scenario_total_count == 0:
            scenario_accuracy = "N/A"
        else:
            scenario_accuracy = f"{scenario_total_correct/scenario_total_count*100:.1f}% ({scenario_total_correct}/{scenario_total_count})"
        row.append(scenario_accuracy)
        
        table.add_row(row)
    
    # æ·»åŠ æ€»è®¡è¡Œ
    total_row = ["æ€»è®¡"]
    overall_total_correct = 0
    overall_total_count = 0
    
    for model in model_names:
        all_correct = sum(correct for correct, total in stats[model].values())
        all_total = sum(total for correct, total in stats[model].values())
        if all_total == 0:
            total_accuracy = "N/A"
        else:
            total_accuracy = f"{all_correct/all_total*100:.1f}% ({all_correct}/{all_total})"
        total_row.append(total_accuracy)
        
        # ç´¯è®¡æ‰€æœ‰åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
        overall_total_correct += all_correct
        overall_total_count += all_total
    
    # è®¡ç®—æ‰€æœ‰åœºæ™¯æ€»è®¡çš„æ€»è®¡
    if overall_total_count == 0:
        overall_accuracy = "N/A"
    else:
        overall_accuracy = f"{overall_total_correct/overall_total_count*100:.1f}% ({overall_total_correct}/{overall_total_count})"
    total_row.append(overall_accuracy)
    
    table.add_row(total_row)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    table.align = "l"  # å·¦å¯¹é½
    table.padding_width = 1  # å•å…ƒæ ¼å†…è¾¹è·
    
    # æ‰“å°è¡¨æ ¼
    print(table)
    print(f"{'='*100}")

if __name__ == "__main__":
    main()