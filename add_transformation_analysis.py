import os
import json
import time
from tqdm import tqdm
from phase0_config_data import client_deepseek, HELPER_MODEL
from phase1_prompt_gen import analyzer_agent  # ä»phase1_prompt_gen.pyå¯¼å…¥analyzer_agent

# å¤åˆ¶å¿…è¦çš„è¾…åŠ©å‡½æ•°
def clean_and_parse_json(content):
    """
    JSON cleaning function to handle extra data and Markdown issues
    """
    if not content: return None
    content = content.strip()
    
    try:
        if "```" in content:
            first_marker = content.find("```")
            last_marker = content.rfind("```")
            if first_marker != -1 and last_marker != -1 and first_marker != last_marker:
                content = content[first_marker + 3 : last_marker].strip()
                content = content.replace("json", "", 1).strip()
        
        brace_count = 0
        start_idx = -1
        end_idx = -1
        
        for i, char in enumerate(content):
            if char == '{':
                if brace_count == 0:
                    start_idx = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i
                    break
        
        if start_idx != -1 and end_idx != -1:
            content = content[start_idx : end_idx + 1]
        
        import re
        content = re.sub(r',\s*([}\]])', r'\1', content)
        
        return json.loads(content)
    except Exception as e:
        print(f"DEBUG: JSON Parse Error - {e}")
        return None

def analyze_missing_entries(data, analyzer_func, file_path=None):
    """
    ä¸ºç¼ºå¤±transformation_analysiså­—æ®µçš„æ¡ç›®è°ƒç”¨åˆ†æå™¨
    
    Args:
        data: è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨
        analyzer_func: åˆ†æå™¨å‡½æ•°
        file_path: æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™å®æ—¶ä¿å­˜
    
    Returns:
        æ›´æ–°åçš„æ•°æ®
    """
    # ç»Ÿè®¡ç¼ºå¤±æ¡ç›®
    missing_entries = [entry for entry in data if 'transformation_analysis' not in entry]
    total = len(missing_entries)
    
    if total == 0:
        print("âœ… No entries missing transformation_analysis field")
        return data
    
    # è®¡æ•°å™¨
    success_count = 0
    error_count = 0
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for i, entry in enumerate(tqdm(missing_entries, desc="Analyzing entries")):
        try:
            # æå–normal_middleå’Œsurreal_middle
            if 'normal_timeline' in entry and entry['normal_timeline'] and 'middle_caption' in entry['normal_timeline']:
                normal_middle = entry['normal_timeline']['middle_caption']
            else:
                normal_middle = None
            
            if 'timeline' in entry and entry['timeline'] and 'middle_caption' in entry['timeline']:
                surreal_middle = entry['timeline']['middle_caption']
            else:
                surreal_middle = None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯è¿›è¡Œåˆ†æ
            if not normal_middle or not surreal_middle:
                print(f"âŒ Skipping entry {i+1}/{total}: Missing normal_middle or surreal_middle")
                error_count += 1
                continue
            
            # è°ƒç”¨åˆ†æå™¨å‡½æ•°ï¼Œæœ€å¤šé‡è¯•3æ¬¡
            for retry in range(3):
                try:
                    # è°ƒç”¨åˆ†æå™¨
                    result = analyzer_func(normal_middle, surreal_middle)
                    
                    # éªŒè¯ç»“æœå¹¶ä¿å­˜åˆ°æ¡ç›®
                    if result and isinstance(result, dict):
                        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¡ç›®å¹¶æ›´æ–°
                        for original_entry in data:
                            if original_entry is entry:
                                original_entry['transformation_analysis'] = result
                                break
                        
                        success_count += 1
                        print(f"âœ… Successfully analyzed entry {i+1}/{total} (Retry: {retry})")
                        
                        # å¦‚æœæä¾›äº†æ–‡ä»¶è·¯å¾„ï¼Œåˆ™æ¯æˆåŠŸåˆ†æä¸€ä¸ªæ¡ç›®å°±ç«‹å³ä¿å­˜
                        if file_path:
                            print(f"ğŸ’¾ Saving after successful analysis of entry {i+1}/{total}...")
                            save_json_file(data, file_path)  # é»˜è®¤backup=False
                        
                        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
                        time.sleep(2)
                        break
                    else:
                        print(f"âš ï¸ Invalid analysis result for entry {i+1}/{total}")
                        if retry < 2:
                            print(f"ğŸ”„ Retrying ({retry + 1}/3)...")
                            time.sleep(2)
                except Exception as e:
                    print(f"âš ï¸ Analysis failed for entry {i+1}/{total}: {str(e)}")
                    if retry < 2:
                        print(f"ğŸ”„ Retrying ({retry + 1}/3)...")
                        time.sleep(2)
            else:
                print(f"âŒ Failed to analyze entry {i+1}/{total} after 3 retries")
                error_count += 1
        except Exception as e:
            print(f"âŒ Unexpected error processing entry {i+1}/{total}: {str(e)}")
            error_count += 1
    
    # æ‰“å°æ€»ç»“
    print(f"\nğŸ“Š Analysis Summary:")
    print(f"âœ… Successful: {success_count}")
    print(f"âŒ Failed: {error_count}")
    print(f"ğŸ“ˆ Success rate: {(success_count / total * 100):.1f}%\n")
    
    return data

def save_json_file(data, file_path, backup=False):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶ï¼Œå¹¶å¯é€‰åœ°åˆ›å»ºå¤‡ä»½
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        file_path: æ–‡ä»¶è·¯å¾„
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½ï¼Œé»˜è®¤ä¸ºFalse
    """
    try:
        # å¦‚æœéœ€è¦å¤‡ä»½ä¸”æ–‡ä»¶å­˜åœ¨
        if backup and os.path.exists(file_path):
            # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
            timestamp = int(time.time())
            backup_path = f"{file_path}.backup_{timestamp}"
            
            # å¤åˆ¶æ–‡ä»¶å†…å®¹ä½œä¸ºå¤‡ä»½
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… Created backup at {backup_path}")
        
        # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Successfully saved data to {file_path}")
        return True
    except Exception as e:
        print(f"âŒ Error saving file: {str(e)}")
        return False

def read_json_file(file_path):
    """
    è¯»å–JSONæ–‡ä»¶
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        è§£æåçš„æ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… Successfully loaded {len(data)} entries from {file_path}")
        return data
    except FileNotFoundError:
        print(f"âŒ Error: File not found at {file_path}")
        return None
    except json.JSONDecodeError:
        print(f"âŒ Error: Invalid JSON format in file {file_path}")
        return None
    except Exception as e:
        print(f"âŒ Error reading JSON file: {str(e)}")
        return None

def check_transformation_analysis(data):
    """
    æ£€æŸ¥æ•°æ®ä¸­ç¼ºå¤±transformation_analysiså­—æ®µçš„æ¡ç›®æ•°é‡
    
    Args:
        data: JSONæ•°æ®åˆ—è¡¨
    
    Returns:
        ç¼ºå¤±å­—æ®µçš„æ¡ç›®æ•°é‡
    """
    return sum(1 for entry in data if 'transformation_analysis' not in entry)

def main():
    """
    ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªå¤„ç†æµç¨‹
    """
    # æ–‡ä»¶è·¯å¾„
    json_file_path = '/Users/tongyao/Documents/VSCode code/v5.2/physibench_surprise_v79_unique.json'
    
    # é¦–å…ˆè¯»å–JSONæ–‡ä»¶
    print("ğŸ“‚ Reading JSON file...")
    data = read_json_file(json_file_path)
    
    if not data:
        print("âŒ Failed to read JSON file. Exiting.")
        return
    
    # åœ¨ç¨‹åºå¼€å§‹æ—¶åˆ›å»ºä¸€æ¬¡å¤‡ä»½
    print("ğŸ“‹ Creating initial backup...")
    save_json_file(data, json_file_path, backup=True)
    
    # æ£€æŸ¥ç¼ºå¤±transformation_analysiså­—æ®µçš„æ¡ç›®
    print("ğŸ” Checking for entries missing transformation_analysis field...")
    missing_entries_count = check_transformation_analysis(data)
    
    print(f"ğŸ“Š Found {missing_entries_count} entries missing transformation_analysis field")
    
    if missing_entries_count > 0:
        # ä¸ºç¼ºå¤±å­—æ®µçš„æ¡ç›®è°ƒç”¨åˆ†æå™¨
        print("ğŸ§  Starting analysis for missing entries...")
        analyze_missing_entries(data, analyzer_agent, json_file_path)
        
        # æœ€åå†ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿æ‰€æœ‰ä¿®æ”¹éƒ½è¢«å†™å…¥
        print("ğŸ’¾ Final save...")
        save_json_file(data, json_file_path)
        
        # éªŒè¯å¤„ç†ç»“æœ
        final_missing = check_transformation_analysis(data)
        print(f"âœ… Process completed. Remaining missing entries: {final_missing}")
        print(f"ğŸ“ˆ Successfully added transformation_analysis to {missing_entries_count - final_missing} entries")
    else:
        print("âœ… All entries already have transformation_analysis field. No changes needed.")
    
    print("ğŸ‰ Task completed successfully!")

if __name__ == "__main__":
    main()