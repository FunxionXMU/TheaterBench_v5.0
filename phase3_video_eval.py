import os
import json
import cv2
import base64
import requests
import re
import numpy as np
import time
import threading
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from phase0_config_data import client_gemini, DIRECTOR_MODEL

# ================= é…ç½®éƒ¨åˆ† =================

# Remove the old API configuration since we're using gemini
# API_KEY = "sk-izrxbwrnxotwsvcngnnmwivxmyqukrivnjcoszzpscmfasjz"
# BASE_URL = "https://api.siliconflow.cn/v1/chat/completions"

# Use the gemini model from config
MODEL_NAME = DIRECTOR_MODEL

# Support for JSON format output
SUPPORT_JSON_MODELS = [DIRECTOR_MODEL]  # Use the model from config

# ğŸŸ¢ ä¿®æ”¹ï¼šä¸å†ç¡¬ç¼–ç è¾“å…¥æ–‡ä»¶ï¼Œè®¾ä¸º None ä»¥ä¾¿è‡ªåŠ¨æŸ¥æ‰¾
INPUT_JSON_FILE = None 
VIDEO_DIR = "t2v_videos"
OUTPUT_JSON_FILE = None  # å°†åœ¨mainå‡½æ•°ä¸­æ ¹æ®ç‰ˆæœ¬å·åŠ¨æ€è®¾ç½®

# ================= è¾…åŠ©å‡½æ•° =================

def sanitize_filename(name):
    """
    ä¸ç”Ÿæˆè§†é¢‘æ—¶çš„å‘½åé€»è¾‘ä¿æŒä¸€è‡´
    """
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    return name.replace(" ", "_")

def call_transit_model(raw_text, is_coherence=False):
    """è°ƒç”¨ä¸­è½¬æ¨¡å‹è½¬æ¢æ ¼å¼ä¸ºJSON
    is_coherence: æ˜¯å¦ä¸ºè¿è´¯æ€§è¯„ä¼°ï¼Œå†³å®šè¿”å›çš„JSONå­—æ®µç»“æ„
    """
    if is_coherence:
        transit_system_prompt = """
        You are a professional text format converter. Your job is to convert the given text into a strict JSON format.
        The JSON should have three fields:
        1. "expected_end_frame": "<Description of what the end frame should look like>"
        2. "reasoning": "<Explanation of the evaluation>"
        3. "score": <0-10, allow decimals>
        OUTPUT FORMAT: Return a STRICT JSON object with no additional text.
        """
        
        transit_user_prompt = f"Please convert the following text into JSON format with expected_end_frame, reasoning, and score fields:\n{raw_text}"
    else:
        transit_system_prompt = """
        You are a professional text format converter. Your job is to convert the given text into a strict JSON format.
        The JSON should have two fields:
        1. "reasoning": "<Short explanation of why>"
        2. "score": <0-10, allow decimals>
        OUTPUT FORMAT: Return a STRICT JSON object with no additional text.
        """
        
        transit_user_prompt = f"Please convert the following text into JSON format:\n{raw_text}"
    
    transit_messages = [
        {"role": "system", "content": transit_system_prompt},
        {"role": "user", "content": transit_user_prompt}
    ]
    
    max_retries = 3
    retry_delay = 5
    
    for retry in range(max_retries):
        try:
            response = client_gemini.chat.completions.create(
                model=DIRECTOR_MODEL,
                messages=transit_messages,
                temperature=0.1,
                max_tokens=1024,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            result = json.loads(content)
            
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
            if is_coherence and 'expected_end_frame' not in result:
                result['expected_end_frame'] = 'No expected end frame description provided'
            return result
        except Exception as e:
            print(f"âŒ Transit Error: {e}")
            
        if retry < max_retries - 1:
            print(f"ğŸ”„ Transit retrying in {retry_delay} seconds... (Attempt {retry + 2}/{max_retries})")
            time.sleep(retry_delay)
            retry_delay *= 2
    
    if is_coherence:
        return {"expected_end_frame": "Transit failed to convert to JSON", "score": 0, "reasoning": "Transit failed to convert to JSON"}
    else:
        return {"score": 0, "reasoning": "Transit failed to convert to JSON"}

def extract_frames(video_path):
    """
    ä»è§†é¢‘ä¸­æå– 3 å¸§ (Start, Mid, End)
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return None

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps if fps > 0 else 0

    timestamps = [0.1, duration / 2, duration-0.1]
    
    extracted_frames = []
    
    for t in timestamps:
        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)
        ret, frame = cap.read()
        if ret:
            height, width = frame.shape[:2]
            new_width = 512
            new_height = int(height * (new_width / width))
            frame_resized = cv2.resize(frame, (new_width, new_height))
            
            _, buffer = cv2.imencode('.jpg', frame_resized)
            b64_str = base64.b64encode(buffer).decode('utf-8')
            extracted_frames.append(b64_str)
        else:
            print(f"âš ï¸ æ— æ³•è¯»å–æ—¶é—´ç‚¹ {t}s çš„å¸§")
    
    cap.release()
    
    if len(extracted_frames) == 3:
        return extracted_frames
    return None

def call_vlm_evaluator(frame, description, segment_name):
    """
    è°ƒç”¨ Qwen-VL å¯¹å•ä¸ªè§†é¢‘ç‰‡æ®µè¿›è¡Œè¯„åˆ†ï¼ŒåŒ…å«é‡è¯•é€»è¾‘
    """
    system_prompt = """
    You are an expert Video Quality Assurance AI. 
    Your job is to evaluate if a specific frame from a generated video matches the required description.
    You will receive one frame from the video and the expected textual description for that specific moment.
    
    YOUR FOCUS: ONLY evaluate whether this specific frame matches its specific description.
    Do NOT consider other frames or descriptions.
    
    CRITERIA:
    1. **Adherence:** Does the visual content match the expected state of the described action at this specific phase?
    2. **Accuracy:** Is the object, scene, or action described clearly visible in this frame?
    
    OUTPUT FORMAT:
    Return a STRICT JSON object:
    {
        "reasoning": "<Short explanation of why>",
        "score": <0-10, allow decimals>
    }
    """

    # æ ¹æ®ç‰‡æ®µç±»å‹æ·»åŠ ç‰¹æ®Šæç¤º
    segment_special_note = ""
    if segment_name == "Start":
        segment_special_note = "âš ï¸  SPECIAL NOTE FOR START FRAME: If the description contains phrases like 'suddenly appears' or similar, it is reasonable that the object is not yet fully visible or present in this frame."
    elif segment_name == "End":
        segment_special_note = "âš ï¸  SPECIAL NOTE FOR END FRAME: If the description contains phrases like 'disappears' or 'merges with something else', it is reasonable that the object is no longer visible or distinguishable in this frame."
    
    user_prompt = f"""
    # ğŸ–¼ï¸ CURRENT SEGMENT TO EVALUATE:
    **Segment:** {segment_name}
    **Expected Description:** {description}
    {segment_special_note}

    # ğŸ–¼ï¸ PROVIDED IMAGE:
    This is the frame from the {segment_name} segment.

    Evaluate if this frame matches the expected description for this specific moment, considering any special notes provided.
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}}
            ]
        }
    ]

    # æ ¹æ®æ¨¡å‹æ˜¯å¦æ”¯æŒJSONæ ¼å¼æ¥å†³å®šæ˜¯å¦è®¾ç½®response_format
    max_retries = 5  # å¢åŠ æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay = 10  # å¢åŠ åˆå§‹å»¶è¿Ÿåˆ°10ç§’
    
    for retry in range(max_retries):
        try:
            response = client_gemini.chat.completions.create(
                model=DIRECTOR_MODEL,
                messages=messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"} if DIRECTOR_MODEL in SUPPORT_JSON_MODELS else None
            )
            
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            
            # å¦‚æœæ¨¡å‹æ”¯æŒJSONæ ¼å¼ï¼Œç›´æ¥è§£æ
            if DIRECTOR_MODEL in SUPPORT_JSON_MODELS:
                return json.loads(content)
            else:
                # å¦åˆ™è°ƒç”¨ä¸­è½¬æ¨¡å‹è½¬æ¢æ ¼å¼
                return call_transit_model(content)
        except Exception as e:
            print(f"âŒ Request Failed: {e}")
            if retry < max_retries - 1:
                print(f"ğŸ”„ Retrying in {retry_delay} seconds... (Attempt {retry + 2}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
            else:
                return {"score": 0, "reasoning": f"Exception: {e}"}
    
    return {"score": 0, "reasoning": "Max retries exceeded"}


def evaluate_coherence(start_frame, end_frame, synopsis, start_description, end_description):
    """
    è¯„ä¼°é¦–å°¾å¸§ä¸æ¢—æ¦‚çš„è¿è´¯æ€§
    é¦–å…ˆåŸºäºé¦–å¸§å’Œæ¢—æ¦‚æ¨ç†å°¾å¸§åº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ç”»é¢ï¼Œ
    ç„¶åè¯„ä¼°ä¸Šä¼ çš„å°¾å¸§ä¸æœŸæœ›æ˜¯å¦ç›¸ç¬¦
    """
    system_prompt = """
    You are an expert Video Quality Assurance AI specializing in narrative coherence evaluation.
    
    Your task is to analyze video frames and evaluate story coherence. You will receive:
    1. A start frame showing the beginning of a story
    2. An end frame showing the conclusion
    3. A synopsis describing the events that should occur
    4. Descriptions of what should be in each frame
    
    YOUR EVALUATION PROCESS:
    1. First, examine the start frame and understand what it shows
    2. Read the synopsis to understand what events should happen
    3. Determine what the end frame SHOULD look like after these events
    4. Examine the actual end frame provided
    5. Compare the expected end frame with the actual end frame
    6. Score how well they match (0-10 scale)
    
    EVALUATION CRITERIA:
    - Logical progression: Does the story make visual sense from start to end?
    - Event completion: Are the synopsis events properly reflected in the frames?
    - Visual consistency: Do the visual elements transform appropriately?
    
    IMPORTANT: You must provide a detailed description of what the end frame should look like based on your analysis of the start frame and synopsis events.
    
    OUTPUT FORMAT - YOU MUST RETURN EXACTLY THIS JSON STRUCTURE:
    {
        "expected_end_frame": "Detailed description of what the end frame should show based on start frame + synopsis",
        "reasoning": "Explanation of your analysis and comparison",
        "score": "Number from 0-10 with 1 decimal place"
    }
    """
    
    user_prompt = f"""
    # ğŸ¬ STORY ANALYSIS TASK
    
    ## STARTING POINT
    **Start Frame Description:** {start_description}
    This is what the video begins with.
    
    ## STORY EVENTS  
    **Synopsis:** {synopsis}
    These events should occur between start and end.
    
    ## EXPECTED CONCLUSION
    **End Frame Description:** {end_description}
    This is what should be shown in the end frame.
    
    ## YOUR TASK
    1. Look at the START frame first - what does it show?
    2. Read the SYNOPSIS - what should happen next?
    3. Determine what the END frame SHOULD look like after these events
    4. Look at the ACTUAL end frame provided
    5. Compare: Does the actual end frame match your expectation?
    6. Score the match (0-10) and explain your reasoning
    
    ## FRAMES TO ANALYZE
    Below are the start and end frames. Analyze them carefully and provide your evaluation in the required JSON format.
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user", 
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{start_frame}"}},
                {"type": "text", "text": "This is the START frame."},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{end_frame}"}},
                {"type": "text", "text": "This is the END frame to evaluate."}
            ]
        }
    ]

    max_retries = 5
    retry_delay = 10
    
    for retry in range(max_retries):
        try:
            response = client_gemini.chat.completions.create(
                model=DIRECTOR_MODEL,
                messages=messages,
                temperature=0.1,
                max_tokens=1024,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            
            if DIRECTOR_MODEL in SUPPORT_JSON_MODELS:
                result = json.loads(content)
                return result
            else:
                return call_transit_model(content, is_coherence=True)
        except Exception as e:
            print(f"âŒ Coherence Request Failed: {e}")
            if retry < max_retries - 1:
                print(f"ğŸ”„ Retrying in {retry_delay} seconds... (Attempt {retry + 2}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                return {"expected_end_frame": "Evaluation failed", "reasoning": f"Exception: {e}", "score": 0}
    
    return {"expected_end_frame": "Evaluation failed", "reasoning": "Max retries exceeded", "score": 0}

# ================= è¾…åŠ©å‡½æ•° - å¹¶è¡Œå¤„ç† =================

def process_entry(entry):
    """å¤„ç†å•ä¸ªæ¡ç›®ï¼Œè¿”å›è¯„ä¼°ç»“æœ"""
    constraints = entry.get('constraints', {})
    keyword = constraints.get('keyword', '')
    s_type = constraints.get('type', '')
    
    safe_keyword = sanitize_filename(keyword)
    # ç¡®ä¿æ­£ç¡®å¤„ç†æ–°çš„åœºæ™¯åï¼Œä¸ Phase 2 ä¿æŒä¸€è‡´
    safe_type = sanitize_filename(s_type.replace(" Scenario", ""))
    
    timeline = entry.get('timeline', {})
    synopsis = entry.get('synopsis', '')
    
    # è·å–ä¸‰ä¸ªç‰‡æ®µçš„æè¿°
    segment_descriptions = {
        "Start": timeline.get("start_caption", "N/A"),
        "Middle": timeline.get("middle_caption", "N/A"),
        "End": timeline.get("end_caption", "N/A")
    }

    print(f"ğŸ”„ Processing {keyword} - {s_type}...")
    
    best_score = -1
    best_video_file = None
    best_reasoning = ""
    candidates = []
    
    found_videos = []
    for i in range(1, 6): 
        filename = f"{safe_keyword}_{safe_type}_{i}.mp4"
        filepath = os.path.join(VIDEO_DIR, filename)
        
        if os.path.exists(filepath):
            found_videos.append((filepath, filename))
    
    if not found_videos:
        eval_entry = {
            "object_name": keyword,
            "scenario_type": s_type,
            "status": "missing_videos"
        }
        return eval_entry
        
    for filepath, filename in found_videos:
        frames = extract_frames(filepath)
        
        if frames and len(frames) == 3:
            # å¯¹æ¯ä¸ªç‰‡æ®µå•ç‹¬è¯„åˆ†
            segment_scores = []
            valid_video = True  # æ ‡è®°è§†é¢‘æ˜¯å¦æœ‰æ•ˆï¼ˆæ‰€æœ‰ç‰‡æ®µéƒ½åŠæ ¼ï¼‰
            
            for i, (segment_name, frame) in enumerate(zip(["Start", "Middle", "End"], frames)):
                description = segment_descriptions[segment_name]
                eval_result = call_vlm_evaluator(frame, description, segment_name)
                
                segment_score = {
                    "segment": segment_name,
                    "score": float(eval_result.get('score', 0)),
                    "reasoning": eval_result.get('reasoning', '')
                }
                segment_scores.append(segment_score)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰‡æ®µä¸åŠæ ¼ï¼ˆ<6åˆ†ï¼‰
                if segment_score["score"] < 6:
                    valid_video = False
            
            # å¦‚æœè§†é¢‘å¸§åŒ¹é…é€šè¿‡ï¼Œå†è¿›è¡Œè¿è´¯æ€§è¯„ä¼°
            if valid_video:
                # è®¡ç®—ä¸‰ä¸ªç‰‡æ®µçš„å¹³å‡åˆ†ä½œä¸ºè§†é¢‘å¸§åŒ¹é…åˆ†æ•°
                frame_match_score = sum(seg["score"] for seg in segment_scores) / len(segment_scores)
                
                # è¿›è¡Œè¿è´¯æ€§è¯„ä¼°
                start_frame, _, end_frame = frames
                coherence_result = evaluate_coherence(
                    start_frame, 
                    end_frame, 
                    synopsis, 
                    segment_descriptions["Start"], 
                    segment_descriptions["End"]
                )
                
                coherence_score = float(coherence_result.get('score', 0))
                
                # æ£€æŸ¥è¿è´¯æ€§æ˜¯å¦åŠæ ¼
                if coherence_score >= 6:
                    # ç»¼åˆåˆ†æ•°ï¼šå¸§åŒ¹é…åˆ†æ•°ï¼ˆå 70%ï¼‰ + è¿è´¯æ€§åˆ†æ•°ï¼ˆå 30%ï¼‰
                    final_score = (frame_match_score * 0.7) + (coherence_score * 0.3)
                    final_reasoning = f"Frame match score: {frame_match_score:.2f}, Coherence score: {coherence_score:.2f}"
                else:
                    # è¿è´¯æ€§ä¸åŠæ ¼ï¼Œè§†é¢‘æ— æ•ˆ
                    valid_video = False
                    final_score = 0
                    final_reasoning = "Video invalid due to failed coherence evaluation."
            else:
                # æœ‰ç‰‡æ®µä¸åŠæ ¼ï¼Œè§†é¢‘ä½œåºŸ
                final_score = 0
                final_reasoning = "Video invalid due to failed segment(s)."
                coherence_result = None
            
            # å‡†å¤‡è¿è´¯æ€§è¯„ä¼°ç»“æœ
            coherence_evaluation = None
            if coherence_result:
                coherence_evaluation = {
                    "expected_end_frame": coherence_result.get('expected_end_frame', ''),
                    "reasoning": coherence_result.get('reasoning', ''),
                    "score": coherence_result.get('score', 0)
                }
            
            candidates.append({
                "video_file": filename,
                "final_score": final_score,
                "segment_scores": segment_scores,
                "coherence_evaluation": coherence_evaluation,
                "reasoning": final_reasoning,
                "valid": valid_video
            })
            
            # æ›´æ–°æœ€ä½³è§†é¢‘ï¼ˆåªè€ƒè™‘æœ‰æ•ˆè§†é¢‘ï¼‰
            if valid_video and final_score > best_score:
                best_score = final_score
                best_video_file = filename
                best_reasoning = final_reasoning
        else:
            candidates.append({
                "video_file": filename,
                "final_score": 0,
                "segment_scores": [],
                "coherence_evaluation": None,
                "reasoning": "Frame extraction failed (must get exactly 3 frames)",
                "valid": False
            })

    eval_entry = {
        "object_name": keyword,
        "scenario_type": s_type,
        "status": "evaluated",
        "best_video": best_video_file,
        "best_score": best_score,
        "best_reasoning": best_reasoning,
        "all_candidates": candidates,
        "has_valid_video": best_video_file is not None  # æ ‡è®°æ˜¯å¦æœ‰æœ‰æ•ˆè§†é¢‘
    }
    
    return eval_entry

# ================= ä¸»ç¨‹åº =================

def main():
    print(f"ï¿½ Starting Video Evaluation using {MODEL_NAME}...")
    
    # ğŸŸ¢ 1. è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ surprise JSON æ–‡ä»¶ (å¦‚æœæ²¡æœ‰æŒ‡å®š)
    target_file = INPUT_JSON_FILE
    if not target_file:
        json_files = [f for f in os.listdir('.') if f.startswith('physibench_surprise_') and f.endswith('.json')]
        json_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        if json_files:
            target_file = json_files[0]
            print(f"ğŸ“‚ Auto-selected latest file: {target_file}")
        else:
            print(f"âŒ No physibench_surprise_*.json files found.")
            return

    if not os.path.exists(target_file):
        print(f"âŒ Input file not found: {target_file}")
        return
    
    # ğŸŸ¢ 2. ä»è¾“å…¥æ–‡ä»¶åä¸­æå–ç‰ˆæœ¬å·
    version_match = re.search(r'v(\d+)', target_file)
    if version_match:
        version = version_match.group(1)
        print(f"ğŸ“Œ Extracted version: v{version}")
    else:
        print(f"âš ï¸  Could not extract version from filename: {target_file}")
        version = "unknown"
    
    # ğŸŸ¢ 3. è®¾ç½®è¾“å‡ºæ–‡ä»¶åï¼Œä½¿ç”¨æå–çš„ç‰ˆæœ¬å·
    global OUTPUT_JSON_FILE
    OUTPUT_JSON_FILE = f"physibench_evaluated_v{version}.json"
    print(f"ğŸ“ Output file will be: {OUTPUT_JSON_FILE}")

    with open(target_file, "r", encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“‚ Loaded {len(data)} entries")
    
    # ğŸ”„ æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²å®Œæˆçš„ç»“æœæ–‡ä»¶ï¼ˆæ–­ç‚¹é‡ç»­ï¼‰
    existing_results = []
    processed_entries = set()
    no_valid_video_count = 0  # ç»Ÿè®¡æ²¡æœ‰æœ€ç»ˆè§†é¢‘çš„promptæ•°é‡
    missing_videos_count = 0  # ç»Ÿè®¡æ²¡æœ‰å¯¹åº”è§†é¢‘çš„promptæ•°é‡
    
    if os.path.exists(OUTPUT_JSON_FILE):
        try:
            with open(OUTPUT_JSON_FILE, "r", encoding='utf-8') as f:
                existing_results = json.load(f)
            
            if existing_results:
                # åˆ›å»ºå·²å¤„ç†æ¡ç›®çš„å”¯ä¸€æ ‡è¯†é›†åˆ
                processed_entries = {f"{entry['object_name']}_{entry['scenario_type']}" for entry in existing_results}
                print(f"ğŸ“ Found existing results: {len(existing_results)} entries already processed")
                
                # ç»Ÿè®¡å·²å­˜åœ¨ç»“æœä¸­çš„å„ç±»æ•°é‡
                for entry in existing_results:
                    if entry.get('status') == 'missing_videos':
                        missing_videos_count += 1
                        no_valid_video_count += 1
                    elif not entry.get('has_valid_video', False):
                        no_valid_video_count += 1
        except json.JSONDecodeError:
            print(f"âš ï¸  Existing results file {OUTPUT_JSON_FILE} is invalid, will start fresh")
    
    # ç­›é€‰æœªå¤„ç†çš„æ¡ç›®
    unprocessed_data = []
    for entry in data:
        constraints = entry.get('constraints', {})
        keyword = constraints.get('keyword', '')
        s_type = constraints.get('type', '')
        entry_id = f"{keyword}_{s_type}"
        
        if entry_id not in processed_entries:
            unprocessed_data.append(entry)
    
    print(f"ğŸ”„ Will process {len(unprocessed_data)} new entries")
    
    # åˆå¹¶ç°æœ‰ç»“æœ
    results = existing_results.copy()
    
    # å¦‚æœæœ‰æœªå¤„ç†çš„æ¡ç›®ï¼Œå¼€å§‹å¤„ç†
    if unprocessed_data:
        # 2. å¹¶è¡Œå¤„ç†æœªå®Œæˆçš„æ¡ç›®
        # è®¾ç½®æœ€å¤§å¹¶å‘æ•°ï¼Œæ ¹æ®APIé™åˆ¶å’Œç³»ç»Ÿèµ„æºè°ƒæ•´
        max_concurrent = 2  # å‡å°‘å¹¶å‘æ•°ä»¥é¿å…APIé€Ÿç‡é™åˆ¶
        print(f"âš™ï¸  Using {max_concurrent} concurrent workers...")
        
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            # æäº¤æ‰€æœ‰æœªå¤„ç†çš„ä»»åŠ¡
            futures = [executor.submit(process_entry, entry) for entry in unprocessed_data]
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(as_completed(futures), total=len(futures), desc="Evaluating Scenarios"):
                try:
                    eval_entry = future.result()
                    results.append(eval_entry)
                    
                    # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰æœ€ç»ˆè§†é¢‘
                    if eval_entry.get('status') == 'missing_videos':
                        missing_videos_count += 1
                        no_valid_video_count += 1
                    elif not eval_entry.get('has_valid_video', False):
                        no_valid_video_count += 1
                    
                    # ä¿å­˜è¿›åº¦ï¼Œæ¯æ¬¡æœ‰æ–°ç»“æœå°±ä¿å­˜
                    with open(OUTPUT_JSON_FILE, "w", encoding='utf-8') as f:
                        json.dump(results, f, indent=2, ensure_ascii=False)
                except Exception as e:
                    print(f"âŒ Error processing entry: {e}")
    else:
        print("âœ… All entries have already been processed!")
        # ä¿å­˜ç°æœ‰ç»“æœï¼ˆç¡®ä¿æ ¼å¼æ­£ç¡®ï¼‰
        with open(OUTPUT_JSON_FILE, "w", encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    # è®¡ç®—å„ç±»promptæ•°é‡
    total_prompts = len(results)
    passed_count = total_prompts - no_valid_video_count
    failed_with_videos_count = no_valid_video_count - missing_videos_count  # æœ‰è§†é¢‘ä½†æ²¡æœ‰æœ‰æ•ˆè§†é¢‘çš„æ•°é‡
    
    print(f"\nâœ… Evaluation Complete! Results saved to {OUTPUT_JSON_FILE}")
    print(f"ğŸ“Š Statistics:")
    print(f"   - Total prompts: {total_prompts}")
    print(f"   - Passed prompts: {passed_count} (have valid videos)")
    print(f"   - Failed prompts: {failed_with_videos_count} (have videos but no valid ones)")
    print(f"   - Missing videos: {missing_videos_count} (no corresponding videos)")

if __name__ == "__main__":
    main()