import os
import json
import base64
import re
import time
import subprocess
from tqdm import tqdm
from openai import OpenAI
from prettytable import PrettyTable
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®éƒ¨åˆ† =================

# APIé…ç½®ï¼ŒæŒ‰æ¨¡å‹æä¾›å•†åˆ†ç±»
API_CONFIGS = {
    "aliyun": {
        "api_key": "sk-7e6b3a62b1f64945a5a4a9347afa5c72",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    },
    "siliconflow": {
        "api_key": "sk-izrxbwrnxotwsvcngnnmwivxmyqukrivnjcoszzpscmfasjz",
        "base_url": "https://api.siliconflow.cn/v1"  # ä¿®æ­£ï¼šå»æ‰äº†æœ«å°¾çš„ /chat/completions
    }
}

# é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
TEST_MODELS = [
    # é˜¿é‡Œäº‘æ¨¡å‹
    "qwen-vl-max-latest",
    "qwen3-vl-30b-a3b-thinking",
    "qwen3-vl-plus",
    "qwen3-vl-flash",
    # SiliconFlowæ¨¡å‹
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    "zai-org/GLM-4.6V"
]

# æ¨¡å‹åˆ°APIæä¾›å•†çš„æ˜ å°„
MODEL_TO_API = {
    "qwen3-vl-plus": "aliyun",
    "qwen3-vl-flash": "aliyun",
    "qwen-vl-max-latest": "aliyun",
    "qwen3-vl-30b-a3b-thinking": "aliyun",
    "Qwen/Qwen3-Omni-30B-A3B-Instruct": "siliconflow",
    "zai-org/GLM-4.6V": "siliconflow",
    "Qwen/Qwen3-VL-30B-A3B-Instruct": "siliconflow"
}

VIDEO_DIR = "t2v_videos"

# æœ€å¤§å¹¶å‘æ•°è®¾ç½®ï¼ˆæ ¹æ®APIé™åˆ¶å’Œç³»ç»Ÿèµ„æºè°ƒæ•´ï¼‰
# è®¾ç½®ä¸ºæ¨¡å‹æ€»æ•°+1ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘èƒ½å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹
MAX_CONCURRENT_MODELS = len(TEST_MODELS) + 1

# ================= è¾…åŠ©å‡½æ•° =================

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    return name.replace(" ", "_")

def compress_video_smart(video_path, target_size_mb=7.0):
    """
    æ™ºèƒ½å‹ç¼©è§†é¢‘ï¼šåªä¿ç•™è§†é¢‘çš„3-7ç§’ç‰‡æ®µï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œå‹ç¼©ä»¥ç¡®ä¿æ–‡ä»¶å¤§å°åˆé€‚ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æå–è§†é¢‘çš„3-7ç§’ç‰‡æ®µï¼ˆå…±4ç§’ï¼‰ä½œä¸ºåˆ†æç›®æ ‡
    2. è¿›è¡Œå¿…è¦çš„å‹ç¼©ä»¥ç¡®ä¿æœ€ç»ˆæ–‡ä»¶å¤§å°ä¸è¶…è¿‡ç›®æ ‡å¤§å°
    3. ä¿æŒè§†é¢‘è´¨é‡ï¼ˆä½¿ç”¨åˆé€‚çš„CRFå€¼ï¼‰åŒæ—¶ä¼˜åŒ–æ–‡ä»¶å¤§å°
    
    å‚æ•°è¯´æ˜ï¼š
    - video_path: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
    - target_size_mb: ç›®æ ‡æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤ä¸º7.0MB
    
    å…³é”®è°ƒæ•´ï¼š
    1. æˆ‘ä»¬å°† target_size_mb é»˜è®¤å€¼è®¾ç½®ä¸º 7.0MBï¼Œå› ä¸º Base64 ç¼–ç ä¼šä½¿æ–‡ä»¶å¤§å°å¢åŠ çº¦ 33%ã€‚
       - 7.0MB * 1.33 â‰ˆ 9.3MB (å®‰å…¨ï¼Œå°äº API çš„ 10MB é™åˆ¶)
       - 9.0MB * 1.33 â‰ˆ 12.0MB (ä¼šæŠ¥é”™ Exceeded limit)
    2. åªä¿ç•™è§†é¢‘çš„3-7ç§’ç‰‡æ®µè¿›è¡Œå¤„ç†å’Œåˆ†æï¼Œè¿™æ ·å¯ä»¥ï¼š
       - æ˜¾è‘—å‡å°æ–‡ä»¶å¤§å°
       - ä¿ç•™è§†é¢‘ä¸­æœ€å¯èƒ½åŒ…å«å…³é”®ä¿¡æ¯çš„éƒ¨åˆ†
       - åŠ å¿«å¤„ç†é€Ÿåº¦
    
    å¤„ç†æµç¨‹ï¼š
    1. é¦–å…ˆæå–3-7ç§’çš„è§†é¢‘ç‰‡æ®µ
    2. å¦‚æœæå–åçš„ç‰‡æ®µä»ç„¶è¶…è¿‡ç›®æ ‡å¤§å°ï¼Œå°è¯•é€šè¿‡é™ä½å¸§ç‡è¿›ä¸€æ­¥å‹ç¼©
    3. ä¿æŒè‰¯å¥½çš„è§†é¢‘è´¨é‡ï¼ˆä½¿ç”¨é€‚å½“çš„CRFå€¼ï¼‰
    
    è¿”å›: (å¤„ç†åçš„æ–‡ä»¶è·¯å¾„, æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶)
    """
    if not os.path.exists(video_path):
        return video_path, False

    file_size = os.path.getsize(video_path) / (1024 * 1024)
    
    # ä¸ç®¡æ–‡ä»¶å¤§å°ï¼Œéƒ½è¿›è¡Œå‹ç¼©å’Œå‰ªè¾‘
    print(f"ğŸ“¦ Video {os.path.basename(video_path)} is {file_size:.2f}MB. Processing...")
    print(f"âœ‚ï¸  Extracting 3-7 second segment as requested...")
    
    temp_path = video_path.replace(".mp4", "_segmented_temp.mp4")
    
    # é¦–å…ˆå‰ªè¾‘è§†é¢‘ï¼Œåªä¿ç•™3-7ç§’éƒ¨åˆ†
    clip_cmd = [
        "ffmpeg", "-y",                # è¦†ç›–è¾“å‡º
        "-i", video_path,              # è¾“å…¥
        "-ss", "3",                   # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
        "-t", "4",                    # æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰- 3-7ç§’ï¼Œå…±4ç§’
        "-c:v", "libx264",             # ç¼–ç å™¨
        "-pix_fmt", "yuv420p",         # å¼ºåˆ¶ä½¿ç”¨ yuv420p åƒç´ æ ¼å¼
        "-crf", "18",                  # æ’å®šè´¨é‡å› å­ (18=é«˜ç”»è´¨/è§†è§‰æ— æŸ)
        "-preset", "veryfast",         # ç¼–ç é€Ÿåº¦
        "-an",                         # ç§»é™¤éŸ³é¢‘
        temp_path
    ]
    
    try:
        # æ‰§è¡Œå‰ªè¾‘ï¼Œé™é»˜è¾“å‡º
        subprocess.run(clip_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        
        # æ£€æŸ¥å‰ªè¾‘åçš„å¤§å°
        new_size = os.path.getsize(temp_path) / (1024 * 1024)
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œå°è¯•è¿›ä¸€æ­¥å‹ç¼© (CRF 23 ä»ç„¶å¾ˆæ¸…æ™°)
        if new_size > target_size_mb:
            print(f"âš ï¸ Segment is still too large ({new_size:.2f}MB). Further compressing...")
            # ç­–ç•¥ï¼šé™ä½å¸§ç‡ï¼Œä½†ä¿æŒé«˜è´¨é‡
            target_fps = "5"
            
            # å¦‚æœæ–‡ä»¶ç‰¹åˆ«å¤§ï¼Œå°è¯•é™ä½åˆ° 3fps
            if new_size > 30:
                target_fps = "3"
                
            compress_cmd = [
                "ffmpeg", "-y", "-i", temp_path,
                "-r", target_fps, "-c:v", "libx264",
                "-pix_fmt", "yuv420p",
                "-crf", "23", "-preset", "veryfast", "-an",
                temp_path + "_compressed.mp4"
            ]
            
            subprocess.run(compress_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
            
            # æ›¿æ¢ä¸ºå‹ç¼©åçš„æ–‡ä»¶
            os.remove(temp_path)
            os.rename(temp_path + "_compressed.mp4", temp_path)
            new_size = os.path.getsize(temp_path) / (1024 * 1024)

        print(f"âœ… Extracted and processed to {new_size:.2f}MB (3-7 second segment)")
        return temp_path, True
        
    except Exception as e:
        print(f"âŒ Processing failed (ffmpeg might be missing): {e}. Using original video.")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return video_path, False

def call_transit_model(raw_text):
    """è°ƒç”¨ä¸­è½¬æ¨¡å‹è½¬æ¢æ ¼å¼"""
    transit_system_prompt = """
    You are a professional text format converter. Your job is to convert the given text into a strict JSON format.
    The JSON should have two fields:
    1. "answer": The selected option letter (e.g., "A", "B", "C", "D")
    2. "reasoning": "<Short explanation>"
    OUTPUT FORMAT: Return a STRICT JSON object with no additional text.
    """
    
    transit_user_prompt = f"Please convert the following text into JSON format:\n{raw_text}"
    
    transit_messages = [
        {"role": "system", "content": transit_system_prompt},
        {"role": "user", "content": transit_user_prompt}
    ]
    
    api_config = API_CONFIGS["siliconflow"]
    transit_client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    for retry in range(3):
        try:
            response = transit_client.chat.completions.create(
                model="Qwen/Qwen3-Next-80B-A3B-Instruct",
                messages=transit_messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"},
                timeout=60
            )
            content = response.choices[0].message.content
            content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
        except Exception as e:
            if retry == 2: return {"answer": "N/A", "reasoning": f"Transit Error: {e}"}
            time.sleep(2)
    return {"answer": "N/A", "reasoning": "Transit failed"}

def call_vlm_for_description(video_path, model_name):
    """è°ƒç”¨ VLM æ¨¡å‹æŸ¥çœ‹è§†é¢‘å¹¶ç”Ÿæˆè¯¦ç»†æè¿°"""
    api_provider = MODEL_TO_API.get(model_name, "siliconflow")
    api_config = API_CONFIGS[api_provider]
    
    client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    # æè¿°ä»»åŠ¡çš„ç³»ç»Ÿæç¤º
    system_prompt = """
    You are an expert video understanding AI. 
    Please carefully analyze the video and provide a detailed description of everything that happens in the video.
    Your description should be comprehensive, sequential, and capture all important visual elements, movements, and interactions.
    """

    # è¯·æ±‚è¯¦ç»†æè¿°çš„ç”¨æˆ·æç¤º
    user_prompt = """
    Please provide a detailed description of everything that happens in this video. Describe the scenes, objects, actions, and any changes that occur over time.
    """

    try:
        with open(video_path, "rb") as f:
            video_bytes = f.read()
            video_base64 = base64.b64encode(video_bytes).decode('utf-8')
    except FileNotFoundError:
        return "Video file not found during read"

    # æ„å»ºè§†é¢‘å†…å®¹éƒ¨åˆ†
    video_url = f"data:video/mp4;base64,{video_base64}"
    
    if api_provider == "aliyun":
        # é˜¿é‡Œäº‘ OpenAI å…¼å®¹æ¨¡å¼æ ¼å¼ (type: video_url)
        video_content = {
            "type": "video_url",
            "video_url": {"url": video_url}
        }
    else:
        # SiliconFlow æ ¼å¼ (å¸¦ fps ç­‰å‚æ•°)
        video_content = {
            "type": "video_url",
            "video_url": {
                "url": video_url,
                "detail": "auto",
                "max_frames": 12,
                "fps": 1
            }
        }

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [{"type": "text", "text": user_prompt}, video_content]}
    ]

    max_retries = 3
    retry_delay = 5
    
    for retry in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=1024,  # å¢åŠ tokensä»¥è·å–æ›´è¯¦ç»†çš„æè¿°
                timeout=60
            )
            
            content = response.choices[0].message.content.strip()
            return content
                
        except Exception as e:
            print(f"âŒ {model_name} Description Request Failed: {e}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                return f"Description Error: {e}"
    
    return "Max retries exceeded for description"

def call_vlm_model(video_path=None, question=None, options=None, model_name=None, video_description=None):
    """è°ƒç”¨ VLM æ¨¡å‹å›ç­”é—®é¢˜
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆç¬¬ä¸€æ¬¡é—®ç­”ä½¿ç”¨ï¼‰
        question: é—®é¢˜æ–‡æœ¬
        options: é€‰é¡¹å­—å…¸
        model_name: æ¨¡å‹åç§°
        video_description: è§†é¢‘æè¿°æ–‡æœ¬ï¼ˆç¬¬äºŒæ¬¡é—®ç­”ä½¿ç”¨ï¼‰
        
    Returns:
        åŒ…å«ç­”æ¡ˆå’Œæ¨ç†çš„å­—å…¸
    """
    SUPPORT_JSON_MODELS = ["Qwen/Qwen3-VL-30B-A3B-Instruct", "Qwen/Qwen3-Omni-30B-A3B-Instruct", "qwen3-vl-plus", "qwen3-vl-flash", "qwen-vl-max-latest"]
    api_provider = MODEL_TO_API.get(model_name, "siliconflow")
    api_config = API_CONFIGS[api_provider]
    
    client = OpenAI(api_key=api_config["api_key"], base_url=api_config["base_url"])
    
    system_prompt = """
    You are an expert video understanding AI. 
    Analyze the information provided and return a JSON object: {"answer": "<option>", "reasoning": "<text>"}
    """

    if video_description is not None:
        # ç¬¬äºŒæ¬¡é—®ç­”ï¼šä½¿ç”¨è§†é¢‘æè¿°
        user_prompt = f"""
        # VIDEO DESCRIPTION:
        {video_description}
        
        # QUESTION:
        **Question:** {question}
        **OPTIONS:**
        {chr(10).join([f"{key}. {value}" for key, value in options.items()])}
        Select the correct answer based on the video description.
        """
        
        # æ„å»ºæ¶ˆæ¯ï¼Œåªæœ‰æ–‡æœ¬ï¼Œæ²¡æœ‰è§†é¢‘
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    else:
        # ç¬¬ä¸€æ¬¡é—®ç­”ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰ï¼šä½¿ç”¨è§†é¢‘
        user_prompt = f"""
        # VIDEO QUESTION:
        **Question:** {question}
        **OPTIONS:**
        {chr(10).join([f"{key}. {value}" for key, value in options.items()])}
        Select the correct answer.
        """

        try:
            with open(video_path, "rb") as f:
                video_bytes = f.read()
                video_base64 = base64.b64encode(video_bytes).decode('utf-8')
        except FileNotFoundError:
            return {"answer": "N/A", "reasoning": "Video file not found during read"}

        # æ„å»ºè§†é¢‘å†…å®¹éƒ¨åˆ†
        video_url = f"data:video/mp4;base64,{video_base64}"
        
        if api_provider == "aliyun":
            # é˜¿é‡Œäº‘ OpenAI å…¼å®¹æ¨¡å¼æ ¼å¼ (type: video_url)
            video_content = {
                "type": "video_url",
                "video_url": {"url": video_url}
            }
        else:
            # SiliconFlow æ ¼å¼ (å¸¦ fps ç­‰å‚æ•°)
            video_content = {
                "type": "video_url",
                "video_url": {
                    "url": video_url,
                    "detail": "auto",
                    "max_frames": 12,
                    "fps": 1
                }
            }

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": [{"type": "text", "text": user_prompt}, video_content]}
        ]

    max_retries = 3
    retry_delay = 5
    
    for retry in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=512,
                response_format={"type": "json_object"} if model_name in SUPPORT_JSON_MODELS else None,
                timeout=60
            )
            
            content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
            
            if model_name in SUPPORT_JSON_MODELS:
                return json.loads(content)
            else:
                return call_transit_model(content)
                
        except Exception as e:
            print(f"âŒ {model_name} Request Failed: {e}")
            if retry < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                return {"answer": "N/A", "reasoning": f"Exception: {e}"}
    
    return {"answer": "N/A", "reasoning": "Max retries exceeded"}

# ================= ä¸»ç¨‹åº =================

def process_video(eval_entry, prompt_index, test_results, lock, processed_videos, test_mode):
    """å¤„ç†å•ä¸ªè§†é¢‘çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ
    å®ç°ä¸¤æ¬¡é—®ç­”æµç¨‹ï¼š
    1. ç¬¬ä¸€æ¬¡ï¼šè®©VLMçœ‹è§†é¢‘å¹¶ç”Ÿæˆè¯¦ç»†æè¿°
    2. ç¬¬äºŒæ¬¡ï¼šåŸºäºç¬¬ä¸€æ¬¡ç”Ÿæˆçš„æè¿°å›ç­”é—®é¢˜
    """
    object_name = eval_entry['object_name']
    scenario_type = eval_entry['scenario_type']
    
    key = f"{object_name}_{scenario_type}"
    if key not in prompt_index:
        return
    
    if eval_entry['status'] != 'evaluated' or not eval_entry.get('best_video'):
        return
    
    prompt_entry = prompt_index[key]
    mcq = prompt_entry['mcq']
    
    best_video_file = eval_entry['best_video']
    original_video_path = os.path.join(VIDEO_DIR, best_video_file)
    
    # ä½¿ç”¨object_name_scenario_typeç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œé¿å…åŒä¸€è§†é¢‘è¢«é‡å¤å¤„ç†
    unique_key = f"{object_name}_{scenario_type}"
    
    # æ£€æŸ¥è¯¥ç»„åˆæ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
    if unique_key in processed_videos:
        print(f"â­ï¸  Skipping {best_video_file} ({object_name} - {scenario_type})... already processed")
        return
    
    # æå‰å°†è§†é¢‘æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…åç»­å¯èƒ½çš„é‡å¤å¤„ç†
    with lock:
        processed_videos.add(unique_key)
    
    print(f"ğŸ”„ Processing {best_video_file} ({object_name} - {scenario_type})...")
    
    # === è§†é¢‘å‹ç¼©å¤„ç† ===
    current_video_path, is_temp_file = compress_video_smart(original_video_path)
    
    # æ ¹æ®æµ‹è¯•æ–¹å¼é€‰æ‹©é¢˜å¹²
    if test_mode == '1':
        question = mcq['question']
    else:
        question = "What most likely happened in the [3-7s] interval according to the video?"
    
    try:
        # ===== ç¬¬ä¸€æ¬¡é—®ç­”ï¼šç”Ÿæˆè§†é¢‘æè¿° =====
        print(f"   ğŸ” First pass: Generating video descriptions...")
        video_descriptions = {}
        
        # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰æ¨¡å‹ç”Ÿæˆæè¿°
        with ThreadPoolExecutor(max_workers=min(MAX_CONCURRENT_MODELS, len(TEST_MODELS))) as executor:
            future_to_model = {
                executor.submit(call_vlm_for_description, current_video_path, model_name): model_name
                for model_name in TEST_MODELS
            }
            
            for future in as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    description = future.result()
                    video_descriptions[model_name] = description
                    print(f"   [{model_name}] Description generated successfully")
                except Exception as e:
                    print(f"   [{model_name}] Error generating description: {e}")
                    video_descriptions[model_name] = f"Error: {e}"
        
        # ===== ç¬¬äºŒæ¬¡é—®ç­”ï¼šåŸºäºæè¿°å›ç­”é—®é¢˜ =====
        print(f"   ğŸ“ Second pass: Answering questions based on descriptions...")
        
        # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰æ¨¡å‹åŸºäºæè¿°å›ç­”é—®é¢˜
        with ThreadPoolExecutor(max_workers=min(MAX_CONCURRENT_MODELS, len(TEST_MODELS))) as executor:
            future_to_model = {}
            # ä¸ºæ¯ä¸ªæ¨¡å‹æäº¤å›ç­”ä»»åŠ¡ï¼Œä½¿ç”¨å…¶è‡ªå·±ç”Ÿæˆçš„æè¿°
            for model_name in TEST_MODELS:
                if model_name in video_descriptions:
                    future_to_model[executor.submit(
                        call_vlm_model, 
                        video_path=None,  # ä¸ä½¿ç”¨è§†é¢‘ï¼Œåªä½¿ç”¨æè¿°
                        question=question, 
                        options=mcq['options'], 
                        model_name=model_name, 
                        video_description=video_descriptions[model_name]
                    )] = model_name
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    model_response = future.result()
                    
                    correct_answer = mcq['correct_answer']
                    model_answer = model_response['answer']
                    is_correct = str(model_answer).upper() == str(correct_answer).upper()
                    
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": question,  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„é—®é¢˜
                            "mcq_options": mcq['options'],
                            "correct_answer": correct_answer,
                            "model_name": model_name,
                            "model_answer": model_answer,
                            "is_correct": is_correct,
                            "model_reasoning": model_response['reasoning'],
                            "video_description": video_descriptions.get(model_name, "No description"),  # ä¿å­˜ç”Ÿæˆçš„æè¿°
                            "original_score": eval_entry['best_score'],
                            "test_mode": test_mode  # è®°å½•æµ‹è¯•æ–¹å¼
                        })
                        
                    print(f"   [{model_name}] Ans: {model_answer} ({'âœ…' if is_correct else 'âŒ'})")
                except Exception as e:
                    print(f"   [{model_name}] Error answering question: {e}")
                    # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
                    with lock:
                        test_results.append({
                            "object_name": object_name,
                            "scenario_type": scenario_type,
                            "video_file": best_video_file,
                            "mcq_question": question,  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„é—®é¢˜
                            "mcq_options": mcq['options'],
                            "correct_answer": mcq['correct_answer'],
                            "model_name": model_name,
                            "model_answer": "N/A",
                            "is_correct": False,
                            "model_reasoning": f"Error: {e}",
                            "video_description": video_descriptions.get(model_name, "No description"),
                            "original_score": eval_entry['best_score'],
                            "test_mode": test_mode  # è®°å½•æµ‹è¯•æ–¹å¼
                        })
    
    finally:
        # === æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ===
        if is_temp_file and os.path.exists(current_video_path):
            try:
                os.remove(current_video_path)
            except OSError:
                pass

def main():
    import threading
    
    print(f"ğŸš€ Starting Video Understanding Test using {', '.join(TEST_MODELS)}...")
    
    eval_files = [f for f in os.listdir('.') if f.startswith('physibench_evaluated_v') and f.endswith('.json')]
    eval_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    if not eval_files:
        print("âŒ No evaluation files found.")
        return
    
    eval_file = eval_files[0]
    version = re.search(r'v(\d+)', eval_file).group(1)
    
    # å¯»æ‰¾å¯¹åº”çš„MCQç»“æœæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒçš„å‘½åæ ¼å¼ï¼ˆå¦‚mcq_blind_test_results_v76_unique.jsonï¼‰
    mcq_files = [f for f in os.listdir('.') if f.startswith('mcq_blind_test_results_') and f.endswith('.json')]
    matched_mcq_files = []
    for f in mcq_files:
        match = re.search(r'v(\d+)', f)
        if match and match.group(1) == version:
            matched_mcq_files.append(f)
    
    if not matched_mcq_files:
        print(f"âŒ No MCQ result files found for version v{version}")
        return
    
    # é€‰æ‹©æœ€æ–°çš„MCQç»“æœæ–‡ä»¶
    mcq_file = max(matched_mcq_files, key=os.path.getmtime)
    
    with open(eval_file, "r", encoding='utf-8') as f: eval_results = json.load(f)
    with open(mcq_file, "r", encoding='utf-8') as f: mcq_results = json.load(f)
    
    # åˆ›å»ºç´¢å¼•ï¼Œæ˜ å°„object_name_scenario_typeåˆ°mcqç»“æœ
    prompt_index = {f"{result['director_data']['object_name']}_{result['director_data']['scenario_type']}": result for result in mcq_results if result.get('blind_test_passed', False)}
    
    # æ·»åŠ æµ‹è¯•æ–¹å¼é€‰æ‹©
    print("\nğŸ” è¯·é€‰æ‹©æµ‹è¯•æ–¹å¼:")
    print("1. ä½¿ç”¨JSONä¸­çš„åŸå§‹é¢˜å¹²")
    print("2. ä½¿ç”¨é€šç”¨é¢˜å¹²: 'What most likely happened in the [3-7s] interval according to the video?'")
    
    test_mode = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    while test_mode not in ['1', '2']:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        test_mode = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    
    test_results = []
    lock = threading.Lock()  # ç”¨äºçº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœåˆ—è¡¨
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æµ‹è¯•ç»“æœæ–‡ä»¶ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    if test_mode == '1':
        output_file = f"video_our_method_v{version}.json"
    else:
        output_file = f"video_our_method_generic_v{version}.json"
    processed_videos = set()
    
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding='utf-8') as f:
                existing_results = json.load(f)
            test_results = existing_results
            
            # æå–å·²å¤„ç†çš„object_name_scenario_typeç»„åˆ
            for result in existing_results:
                unique_key = f"{result['object_name']}_{result['scenario_type']}"
                processed_videos.add(unique_key)
            
            print(f"ğŸ“‹ Found existing results file: {output_file}")
            print(f"   - {len(existing_results)} results loaded")
            print(f"   - {len(processed_videos)} unique object-scenario combinations already processed")
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  Failed to load existing results: {e}")
            print("   Starting fresh...")
    
    # è®¡ç®—æ€»çš„å¹¶å‘æ•°
    # æ¯ä¸ªè§†é¢‘éœ€è¦å¤„ç†len(TEST_MODELS)ä¸ªæ¨¡å‹ï¼Œæ‰€ä»¥æ€»çš„å¹¶å‘æ•°åº”è¯¥æ˜¯MAX_CONCURRENT_MODELS * è§†é¢‘å¹¶å‘æ•°
    # é™ä½å¹¶å‘æ•°ä»¥é¿å…APIé€Ÿç‡é™åˆ¶ (TPM limit reached)
    video_concurrency = 2
    total_concurrent_workers = video_concurrency
    
    print(f"âš™ï¸  Using {total_concurrent_workers} concurrent video workers, each with up to {MAX_CONCURRENT_MODELS} model workers...")
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰è§†é¢‘
    with ThreadPoolExecutor(max_workers=total_concurrent_workers) as executor:
        # æäº¤æ‰€æœ‰è§†é¢‘å¤„ç†ä»»åŠ¡
        futures = []
        valid_entries = 0
        for eval_entry in eval_results:
            object_name = eval_entry['object_name']
            scenario_type = eval_entry['scenario_type']
            key = f"{object_name}_{scenario_type}"
            
            # åªæäº¤æœ‰æ•ˆçš„è¯„ä¼°æ¡ç›®
            if key in prompt_index and eval_entry['status'] == 'evaluated' and eval_entry.get('best_video'):
                future = executor.submit(process_video, eval_entry, prompt_index, test_results, lock, processed_videos, test_mode)
                futures.append(future)
                valid_entries += 1
        
        print(f"ğŸ“‹ Found {valid_entries} valid evaluation entries matching MCQ blind test results...")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print(f"ğŸ“‹ Submitted {len(futures)} video processing tasks...")
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Videos"):
            try:
                future.result()
            except Exception as e:
                print(f"âŒ Video processing error: {e}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if test_results:
        correct_count = sum(1 for r in test_results if r['is_correct'])
        print(f"\nğŸ“Š Overall Accuracy: {correct_count / len(test_results) * 100:.1f}%")
        
    # ä¿å­˜
    with open(output_file, "w", encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Results saved to {output_file}")
    
    # ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡
    if not test_results:
        print("\nâŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç»Ÿè®¡")
        return
    
    print(f"\nğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸‹çš„æ­£ç¡®ç‡...")
    
    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„åœºæ™¯ç±»å‹å’Œæ¨¡å‹åç§°
    scenario_types = sorted(set(r['scenario_type'] for r in test_results))
    model_names = sorted(set(r['model_name'] for r in test_results))
    
    # ç»Ÿè®¡æ•°æ®ç»“æ„: {model: {scenario: (correct_count, total_count)}}
    stats = {}
    for model in model_names:
        stats[model] = {}
        for scenario in scenario_types:
            # ç­›é€‰è¯¥æ¨¡å‹åœ¨è¯¥åœºæ™¯ä¸‹çš„æ‰€æœ‰ç»“æœ
            model_scenario_results = [
                r for r in test_results 
                if r['model_name'] == model and r['scenario_type'] == scenario
            ]
            if not model_scenario_results:
                stats[model][scenario] = (0, 0)
                continue
            
            total = len(model_scenario_results)
            correct = sum(1 for r in model_scenario_results if r['is_correct'])
            stats[model][scenario] = (correct, total)
    
    # ä½¿ç”¨ PrettyTable æ‰“å°ç»Ÿè®¡è¡¨æ ¼
    print(f"\n{'='*100}")
    print("ğŸ“ˆ å„æ¨¡å‹åœ¨å„åœºæ™¯ä¸‹çš„æ­£ç¡®ç‡ç»Ÿè®¡")
    print(f"{'='*100}")
    
    # åˆ›å»ºè¡¨æ ¼
    table = PrettyTable()
    
    # è®¾ç½®è¡¨å¤´
    field_names = ["åœºæ™¯ç±»å‹"]
    for model in model_names:
        field_names.append(model)
    field_names.append("æƒ…æ™¯æ€»è®¡")  # æ·»åŠ æƒ…æ™¯æ€»è®¡åˆ—
    table.field_names = field_names
    
    # å¡«å……è¡¨æ ¼å†…å®¹
    for scenario in scenario_types:
        row = [scenario]
        scenario_total_correct = 0
        scenario_total_count = 0
        
        for model in model_names:
            correct, total = stats[model][scenario]
            if total == 0:
                accuracy = "N/A"
            else:
                accuracy = f"{correct/total*100:.1f}% ({correct}/{total})"
            row.append(accuracy)
            
            # ç´¯è®¡åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
            scenario_total_correct += correct
            scenario_total_count += total
        
        # è®¡ç®—åœºæ™¯æ€»è®¡
        if scenario_total_count == 0:
            scenario_accuracy = "N/A"
        else:
            scenario_accuracy = f"{scenario_total_correct/scenario_total_count*100:.1f}% ({scenario_total_correct}/{scenario_total_count})"
        row.append(scenario_accuracy)
        
        table.add_row(row)
    
    # æ·»åŠ æ€»è®¡è¡Œ
    total_row = ["æ€»è®¡"]
    overall_total_correct = 0
    overall_total_count = 0
    
    for model in model_names:
        all_correct = sum(correct for correct, total in stats[model].values())
        all_total = sum(total for correct, total in stats[model].values())
        if all_total == 0:
            total_accuracy = "N/A"
        else:
            total_accuracy = f"{all_correct/all_total*100:.1f}% ({all_correct}/{all_total})"
        total_row.append(total_accuracy)
        
        # ç´¯è®¡æ‰€æœ‰åœºæ™¯çš„æ€»æ­£ç¡®æ•°å’Œæ€»æµ‹è¯•æ•°
        overall_total_correct += all_correct
        overall_total_count += all_total
    
    # è®¡ç®—æ‰€æœ‰åœºæ™¯æ€»è®¡çš„æ€»è®¡
    if overall_total_count == 0:
        overall_accuracy = "N/A"
    else:
        overall_accuracy = f"{overall_total_correct/overall_total_count*100:.1f}% ({overall_total_correct}/{overall_total_count})"
    total_row.append(overall_accuracy)
    
    table.add_row(total_row)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    table.align = "l"  # å·¦å¯¹é½
    table.padding_width = 1  # å•å…ƒæ ¼å†…è¾¹è·
    
    # æ‰“å°è¡¨æ ¼
    print(table)
    print(f"{'='*100}")

if __name__ == "__main__":
    main()